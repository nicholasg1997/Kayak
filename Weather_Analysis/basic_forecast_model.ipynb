{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:03.013798Z",
     "end_time": "2023-06-13T11:24:03.021576Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "from meteostat import Stations, Daily\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics import R2Score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "path = \"RawData\"\n",
    "\n",
    "\n",
    "def extract_date_time(filename):\n",
    "    \"\"\"\n",
    "    extract the date and time from the filename\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parts = filename.split('.')\n",
    "    date = parts[1]\n",
    "    time = parts[2]\n",
    "    return date, time\n",
    "\n",
    "\n",
    "def get_date(df, file):\n",
    "    \"\"\"get the date from the dataframe and the time from the filename and combine them into a datetime object\n",
    "    :param df: dataframe containing the date\n",
    "    :param file: filename containing the time\n",
    "    :return: datetime object\n",
    "    \"\"\"\n",
    "    #date_str = df[df.iloc[:, 2] == 1].iloc[0]['Date']\n",
    "    date_str = str(file.split('.')[1])\n",
    "    time_str = str(file.split('.')[2])\n",
    "    #date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    date = datetime.strptime(date_str, '%Y%m%d')\n",
    "    time_value = time(int(time_str), 0)\n",
    "    combined_datetime = datetime.combine(date.date(), time_value)\n",
    "    return combined_datetime\n",
    "\n",
    "\n",
    "degree_days = 'gw_hdd'\n",
    "ecmwf_files = glob.glob(path + f'/ecmwf.*.[01][02].{degree_days}.csv')\n",
    "ecmwf_sorted_files = sorted(ecmwf_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[3:]\n",
    "\n",
    "ecmwf_eps_files = glob.glob(path + f'/ecmwf-eps.*.[01][02].{degree_days}.csv')\n",
    "ecmwf_eps_sorted_files = sorted(ecmwf_eps_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "\n",
    "gfs_ens_bc_files = glob.glob(path + f'/gfs-ens-bc.*.[01][02].{degree_days}.csv')\n",
    "gfs_ens_bc_sorted_files = sorted(gfs_ens_bc_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "\n",
    "cmc_ens_files = glob.glob(path + f'/cmc-ens.*.[01][02].{degree_days}.csv')\n",
    "cmc_ens_sorted_files = sorted(cmc_ens_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "for _ in range(2):\n",
    "    set1 = set((extract_date_time(filename) for filename in ecmwf_sorted_files))\n",
    "    set2 = set((extract_date_time(filename) for filename in ecmwf_eps_sorted_files))\n",
    "\n",
    "    ecmwf_sorted_files = [filename for filename in ecmwf_sorted_files if extract_date_time(filename) in set2]\n",
    "    ecmwf_eps_sorted_files = [filename for filename in ecmwf_eps_sorted_files if extract_date_time(filename) in set1]\n",
    "    cmc_ens_sorted_files = [filename for filename in cmc_ens_sorted_files if extract_date_time(filename) in set1]\n",
    "\n",
    "    master_set = set((extract_date_time(filename) for filename in cmc_ens_sorted_files))\n",
    "    gfs_ens_bc_sorted_files = [filename for filename in gfs_ens_bc_sorted_files if\n",
    "                               extract_date_time(filename) in master_set]\n",
    "\n",
    "    master_set = set((extract_date_time(filename) for filename in gfs_ens_bc_sorted_files))\n",
    "\n",
    "    ecmwf_sorted_files = [filename for filename in ecmwf_sorted_files if extract_date_time(filename) in master_set]\n",
    "    ecmwf_eps_sorted_files = [filename for filename in ecmwf_eps_sorted_files if\n",
    "                              extract_date_time(filename) in master_set]\n",
    "    gfs_ens_bc_sorted_files = [filename for filename in gfs_ens_bc_sorted_files if\n",
    "                               extract_date_time(filename) in master_set]\n",
    "    cmc_ens_sorted_files = [filename for filename in cmc_ens_sorted_files if extract_date_time(filename) in master_set]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:03.206037Z",
     "end_time": "2023-06-13T11:24:04.250884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on 2020-08-15 12:00:00\n",
      "error on 2020-08-16 00:00:00\n",
      "error on 2020-08-16 12:00:00\n",
      "error on 2020-08-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "ecmwf_eps_change_df = pd.DataFrame(columns=['ecmwf-eps_9', 'ecmwf-eps_10', 'ecmwf-eps_11', 'ecmwf-eps_12',\n",
    "                                  'ecmwf-eps_13', 'ecmwf-eps_14'])\n",
    "passed_rows = []\n",
    "for i in range(1, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            changes.append(ecmwf_eps_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=ecmwf_eps_change_df.columns, index=[date])\n",
    "        ecmwf_eps_change_df = pd.concat([ecmwf_eps_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:04.259071Z",
     "end_time": "2023-06-13T11:24:14.931269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "[1494, 1495, 1496, 1497]"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:14.932695Z",
     "end_time": "2023-06-13T11:24:14.935920Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:14.936998Z",
     "end_time": "2023-06-13T11:24:14.938467Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on row: 1494\n",
      "error on row: 1495\n",
      "error on row: 1496\n",
      "error on row: 1497\n"
     ]
    }
   ],
   "source": [
    "ecmwf_change_df = pd.DataFrame(columns=['ecmwf_diff_8', 'ecmwf_diff_9',])\n",
    "passed_rows = []\n",
    "for i in range(1, len(ecmwf_sorted_files)):\n",
    "    ecmwf_df = pd.read_csv(ecmwf_sorted_files[i])\n",
    "    ecmwf_df = ecmwf_df[ecmwf_df[ecmwf_df.columns[2]] >= 1]\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    try:\n",
    "        ecmwf = ecmwf_df.iloc[8]\n",
    "        ecmwf_eps = ecmwf_eps_df.iloc[9]\n",
    "    except IndexError:\n",
    "        print(f\"error on row: {i}\")\n",
    "        passed_rows.append(i)\n",
    "        continue\n",
    "\n",
    "    date = get_date(ecmwf_df, ecmwf_sorted_files[i])\n",
    "    prev_date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8,10):\n",
    "            changes.append(ecmwf_df.iloc[day - offset]['Value'] - ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=ecmwf_change_df.columns, index=[date])\n",
    "        ecmwf_change_df = pd.concat([ecmwf_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:14.943415Z",
     "end_time": "2023-06-13T11:24:23.048401Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "[1494, 1495, 1496, 1497]"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:23.049276Z",
     "end_time": "2023-06-13T11:24:23.051727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:23.052755Z",
     "end_time": "2023-06-13T11:24:23.054757Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:23.055060Z",
     "end_time": "2023-06-13T11:24:23.061270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on 2020-08-15 12:00:00\n",
      "error on 2020-08-16 00:00:00\n",
      "error on 2020-08-16 12:00:00\n",
      "error on 2020-08-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "gfs_ens_bc_change_df = pd.DataFrame(columns=['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12',\n",
    "                                  'gfs-ens-bc_13', 'gfs-ens-bc_14'])\n",
    "passed_rows = []\n",
    "for i in range(1, len(gfs_ens_bc_sorted_files)):\n",
    "    gfs_ens_bc_df = pd.read_csv(gfs_ens_bc_sorted_files[i])\n",
    "    gfs_ens_bc_df = gfs_ens_bc_df[gfs_ens_bc_df[gfs_ens_bc_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    try:\n",
    "        date = get_date(gfs_ens_bc_df, gfs_ens_bc_sorted_files[i])\n",
    "        prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    except IndexError:\n",
    "        print(f\"error on row: {i}\")\n",
    "        passed_rows.append(i)\n",
    "        continue\n",
    "\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            changes.append(gfs_ens_bc_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=gfs_ens_bc_change_df.columns, index=[date])\n",
    "        gfs_ens_bc_change_df = pd.concat([gfs_ens_bc_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:23.063050Z",
     "end_time": "2023-06-13T11:24:33.192438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "data": {
      "text/plain": "[1494, 1495, 1496, 1497]"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:33.195173Z",
     "end_time": "2023-06-13T11:24:33.198008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on 2020-08-15 12:00:00\n",
      "error on 2020-08-16 12:00:00\n"
     ]
    }
   ],
   "source": [
    "cmc_ens_change_df = pd.DataFrame(columns=['cmc-ens_9', 'cmc-ens_10', 'cmc-ens_11', 'cmc-ens_12',\n",
    "                                  'cmc-ens_13', 'cmc-ens_14'])\n",
    "passed_rows = []\n",
    "\n",
    "for i in range(1, len(cmc_ens_sorted_files)):\n",
    "    cmc_ens_df = pd.read_csv(cmc_ens_sorted_files[i])\n",
    "    cmc_ens_df = cmc_ens_df[cmc_ens_df[cmc_ens_df.columns[2]] >= 1]\n",
    "    gfs_ens_bc_df = pd.read_csv(gfs_ens_bc_sorted_files[i])\n",
    "    gfs_ens_bc_df = gfs_ens_bc_df[gfs_ens_bc_df[gfs_ens_bc_df.columns[2]] >= 1]\n",
    "    date = get_date(cmc_ens_df, cmc_ens_sorted_files[i])\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            changes.append(cmc_ens_df.iloc[day]['Value'] - gfs_ens_bc_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=cmc_ens_change_df.columns, index=[date])\n",
    "        cmc_ens_change_df = pd.concat([cmc_ens_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:33.201528Z",
     "end_time": "2023-06-13T11:24:42.884228Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "data": {
      "text/plain": "[1494, 1496]"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:42.884829Z",
     "end_time": "2023-06-13T11:24:42.886995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on 2020-08-15 12:00:00\n",
      "error on 2020-08-16 00:00:00\n",
      "error on 2020-08-16 12:00:00\n",
      "error on 2020-08-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "day_8_error = pd.DataFrame(columns=['day_8_error'])\n",
    "passed_rows = []\n",
    "\n",
    "for i in range(1, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "    day = 7\n",
    "    changes = []\n",
    "    try:\n",
    "        changes.append(ecmwf_eps_df.iloc[day]['Value'] - prev_ecmwf_eps_df.iloc[day + offset]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=day_8_error.columns, index=[date])\n",
    "        day_8_error = pd.concat([day_8_error, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:42.891834Z",
     "end_time": "2023-06-13T11:24:50.527662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error on 2020-08-16 00:00:00\n",
      "error on 2020-08-16 12:00:00\n",
      "error on 2020-08-17 00:00:00\n",
      "error on 2020-08-17 12:00:00\n"
     ]
    }
   ],
   "source": [
    "errors_df = pd.DataFrame(columns=['error_9', 'error_10', 'error_11', 'error_12', 'error_13', 'error_14'])\n",
    "passed_rows = []\n",
    "\n",
    "for i in range(2, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-2])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    errors = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            errors.append(ecmwf_eps_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([errors], columns=errors_df.columns, index=[date])\n",
    "        errors_df = pd.concat([errors_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:50.531896Z",
     "end_time": "2023-06-13T11:24:58.983337Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "new features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "#add if noon"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:58.983779Z",
     "end_time": "2023-06-13T11:24:58.985775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:58.986071Z",
     "end_time": "2023-06-13T11:24:58.988014Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:58.988570Z",
     "end_time": "2023-06-13T11:24:58.990188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "errors_df['noon'] = errors_df.index.hour\n",
    "errors_df['noon'] = errors_df['noon'].apply(lambda x: 1 if x == 12 else 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:58.991315Z",
     "end_time": "2023-06-13T11:24:58.996223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:58.996493Z",
     "end_time": "2023-06-13T11:24:58.998860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = 1 - (abs(x-7) / 7)\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:58.999998Z",
     "end_time": "2023-06-13T11:24:59.002223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "errors_df['month'] = errors_df.index.month\n",
    "errors_df['month'] = errors_df['month'].apply(lambda x: f(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.005714Z",
     "end_time": "2023-06-13T11:24:59.025378Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.008275Z",
     "end_time": "2023-06-13T11:24:59.025527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "                     gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  \\\n2023-05-15 00:00:00         1.370          0.961          0.180   \n2023-05-15 12:00:00        -1.058         -0.058          0.440   \n2023-05-16 00:00:00        -0.018         -0.324         -0.084   \n2023-05-16 12:00:00        -0.719         -0.732         -0.481   \n2023-05-17 00:00:00        -0.135         -0.121         -0.083   \n\n                     gfs-ens-bc_12  gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  \\\n2023-05-15 00:00:00          0.107          0.123          0.015     -0.752   \n2023-05-15 12:00:00          0.334          0.098         -0.206     -0.139   \n2023-05-16 00:00:00          0.108          0.380          0.401      0.330   \n2023-05-16 12:00:00         -0.186          0.040          0.155     -0.360   \n2023-05-17 00:00:00         -0.034         -0.196         -0.119      0.325   \n\n                     cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  error_14  noon  \\\n2023-05-15 00:00:00      -0.071      -0.003       0.156  ...    -0.039   0.0   \n2023-05-15 12:00:00      -0.626      -0.916      -0.714  ...    -0.210   1.0   \n2023-05-16 00:00:00      -0.207      -0.039       0.085  ...    -0.251   0.0   \n2023-05-16 12:00:00      -0.185      -0.309      -0.252  ...     0.179   1.0   \n2023-05-17 00:00:00       0.155       0.067      -0.282  ...     0.379   0.0   \n\n                        month  day_8_error  ecmwf-eps_9  ecmwf-eps_10  \\\n2023-05-15 00:00:00  0.714286        0.985        0.985         0.827   \n2023-05-15 12:00:00  0.714286       -0.607       -0.310         0.312   \n2023-05-16 00:00:00  0.714286        0.375        0.375         0.181   \n2023-05-16 12:00:00  0.714286       -0.624       -0.647        -0.716   \n2023-05-17 00:00:00  0.714286        0.482        0.482         0.299   \n\n                     ecmwf-eps_11  ecmwf-eps_12  ecmwf-eps_13  ecmwf-eps_14  \n2023-05-15 00:00:00         0.163        -0.005        -0.099        -0.262  \n2023-05-15 12:00:00         0.240        -0.036        -0.199        -0.433  \n2023-05-16 00:00:00         0.348         0.421         0.353         0.264  \n2023-05-16 12:00:00        -0.510        -0.097         0.294         0.225  \n2023-05-17 00:00:00         0.191        -0.095        -0.359        -0.209  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>error_14</th>\n      <th>noon</th>\n      <th>month</th>\n      <th>day_8_error</th>\n      <th>ecmwf-eps_9</th>\n      <th>ecmwf-eps_10</th>\n      <th>ecmwf-eps_11</th>\n      <th>ecmwf-eps_12</th>\n      <th>ecmwf-eps_13</th>\n      <th>ecmwf-eps_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-05-15 00:00:00</th>\n      <td>1.370</td>\n      <td>0.961</td>\n      <td>0.180</td>\n      <td>0.107</td>\n      <td>0.123</td>\n      <td>0.015</td>\n      <td>-0.752</td>\n      <td>-0.071</td>\n      <td>-0.003</td>\n      <td>0.156</td>\n      <td>...</td>\n      <td>-0.039</td>\n      <td>0.0</td>\n      <td>0.714286</td>\n      <td>0.985</td>\n      <td>0.985</td>\n      <td>0.827</td>\n      <td>0.163</td>\n      <td>-0.005</td>\n      <td>-0.099</td>\n      <td>-0.262</td>\n    </tr>\n    <tr>\n      <th>2023-05-15 12:00:00</th>\n      <td>-1.058</td>\n      <td>-0.058</td>\n      <td>0.440</td>\n      <td>0.334</td>\n      <td>0.098</td>\n      <td>-0.206</td>\n      <td>-0.139</td>\n      <td>-0.626</td>\n      <td>-0.916</td>\n      <td>-0.714</td>\n      <td>...</td>\n      <td>-0.210</td>\n      <td>1.0</td>\n      <td>0.714286</td>\n      <td>-0.607</td>\n      <td>-0.310</td>\n      <td>0.312</td>\n      <td>0.240</td>\n      <td>-0.036</td>\n      <td>-0.199</td>\n      <td>-0.433</td>\n    </tr>\n    <tr>\n      <th>2023-05-16 00:00:00</th>\n      <td>-0.018</td>\n      <td>-0.324</td>\n      <td>-0.084</td>\n      <td>0.108</td>\n      <td>0.380</td>\n      <td>0.401</td>\n      <td>0.330</td>\n      <td>-0.207</td>\n      <td>-0.039</td>\n      <td>0.085</td>\n      <td>...</td>\n      <td>-0.251</td>\n      <td>0.0</td>\n      <td>0.714286</td>\n      <td>0.375</td>\n      <td>0.375</td>\n      <td>0.181</td>\n      <td>0.348</td>\n      <td>0.421</td>\n      <td>0.353</td>\n      <td>0.264</td>\n    </tr>\n    <tr>\n      <th>2023-05-16 12:00:00</th>\n      <td>-0.719</td>\n      <td>-0.732</td>\n      <td>-0.481</td>\n      <td>-0.186</td>\n      <td>0.040</td>\n      <td>0.155</td>\n      <td>-0.360</td>\n      <td>-0.185</td>\n      <td>-0.309</td>\n      <td>-0.252</td>\n      <td>...</td>\n      <td>0.179</td>\n      <td>1.0</td>\n      <td>0.714286</td>\n      <td>-0.624</td>\n      <td>-0.647</td>\n      <td>-0.716</td>\n      <td>-0.510</td>\n      <td>-0.097</td>\n      <td>0.294</td>\n      <td>0.225</td>\n    </tr>\n    <tr>\n      <th>2023-05-17 00:00:00</th>\n      <td>-0.135</td>\n      <td>-0.121</td>\n      <td>-0.083</td>\n      <td>-0.034</td>\n      <td>-0.196</td>\n      <td>-0.119</td>\n      <td>0.325</td>\n      <td>0.155</td>\n      <td>0.067</td>\n      <td>-0.282</td>\n      <td>...</td>\n      <td>0.379</td>\n      <td>0.0</td>\n      <td>0.714286</td>\n      <td>0.482</td>\n      <td>0.482</td>\n      <td>0.299</td>\n      <td>0.191</td>\n      <td>-0.095</td>\n      <td>-0.359</td>\n      <td>-0.209</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_df = pd.concat([gfs_ens_bc_change_df, cmc_ens_change_df, ecmwf_change_df, errors_df, day_8_error, ecmwf_eps_change_df], axis=1)\n",
    "master_df.fillna(0, inplace=True)\n",
    "display(master_df[-5:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.011977Z",
     "end_time": "2023-06-13T11:24:59.031390Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "master_df.to_pickle('master_df.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.030576Z",
     "end_time": "2023-06-13T11:24:59.034290Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "master_df = pd.read_pickle('master_df.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:06.881484Z",
     "end_time": "2023-06-07T12:27:06.888623Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:13.732847Z",
     "end_time": "2023-06-07T10:51:13.735675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = master_df.iloc[:, :-6]\n",
    "y = master_df.iloc[:, -6:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:14.008246Z",
     "end_time": "2023-06-07T10:51:14.013047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:14.671726Z",
     "end_time": "2023-06-07T10:51:14.674477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:14.851932Z",
     "end_time": "2023-06-07T10:52:01.639354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:52:01.641000Z",
     "end_time": "2023-06-07T10:52:02.138638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_features = master_df.iloc[:, :-6].values ** 2\n",
    "target_variables = master_df.iloc[:, -6:].values\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_features, target_variables, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale the input features based on the training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test data based on the training data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data to PyTorch tensors\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train_scaled)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "X_val_tensor = torch.Tensor(X_val_scaled)\n",
    "y_val_tensor = torch.Tensor(y_val)\n",
    "X_test_tensor = torch.Tensor(X_test_scaled)\n",
    "y_test_tensor = torch.Tensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:57:17.393078Z",
     "end_time": "2023-06-07T11:57:17.402574Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#benchmark error\n",
    "total_mse = 0\n",
    "c = 0\n",
    "for i in range(1, len(y_test_tensor)):\n",
    "    #mse = mean_squared_error(y_test_tensor[i], y_test_tensor[i-1])\n",
    "    mse = mean_squared_error(y_test_tensor[i], [0,0,0,0,0,0])\n",
    "    total_mse += mse\n",
    "    c += 1\n",
    "\n",
    "total_mse/c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:57:17.571212Z",
     "end_time": "2023-06-07T11:57:17.642334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, output_size)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = F.relu(out)  # Apply ReLU activation between LSTM and first dense layer\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = F.relu(out)  # Apply ReLU activation to the output of the first dense layer\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:57:18.215284Z",
     "end_time": "2023-06-07T11:57:18.221355Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = y_train_tensor.shape[1]\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "dropout = 0.3\n",
    "lr = 0.01\n",
    "mps_device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:58:01.427352Z",
     "end_time": "2023-06-07T11:58:01.432498Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:04:37.978958Z",
     "end_time": "2023-06-07T11:04:37.991726Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "sequence_length = 10  # Number of previous days to consider\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loop through each sequence in the training data\n",
    "    for i in range(sequence_length, X_train_tensor.shape[0]):\n",
    "        # Extract the current sequence and target\n",
    "        input_seq = X_train_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "        target_seq = y_train_tensor[i]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / (X_train_tensor.shape[0] - sequence_length)\n",
    "    train_losses.append(average_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {average_loss}')\n",
    "\n",
    "    # Validation stage\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for i in range(sequence_length, X_val_tensor.shape[0]):\n",
    "            input_seq = X_val_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "            target_seq = y_val_tensor[i]\n",
    "\n",
    "            output = model(input_seq)\n",
    "            val_loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / (X_val_tensor.shape[0] - sequence_length)\n",
    "        val_losses.append(average_val_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {average_val_loss}')\n",
    "\n",
    "        # Check if current model is the best based on validation loss\n",
    "        if average_val_loss < best_loss:\n",
    "            best_loss = average_val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "# After training, use the best model for testing\n",
    "model = best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:52:26.272553Z",
     "end_time": "2023-06-07T10:59:06.610026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(train_losses, label='Training Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "plot_losses(train_losses, val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:40.798333Z",
     "end_time": "2023-06-07T11:29:40.955675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(sequence_length, X_test_tensor.shape[0]):\n",
    "        input_seq = X_test_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "        target_seq = y_test_tensor[i]\n",
    "\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Extract the scalar value from the tensor and append it to predictions\n",
    "        predictions.append(output.squeeze().tolist())\n",
    "\n",
    "    average_test_loss = test_loss / (X_test_tensor.shape[0] - sequence_length)\n",
    "    print(f'Test Loss: {average_test_loss}')\n",
    "\n",
    "    # Convert the predictions and target values to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    targets = y_test_tensor[sequence_length:].numpy()\n",
    "\n",
    "    # Evaluate the performance using appropriate metrics\n",
    "    # For example, calculate mean squared error (MSE)\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    metric = R2Score()\n",
    "    r2 = metric.update(torch.tensor(predictions), torch.tensor(targets)).compute()\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'R2 Score: {r2}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:42.167101Z",
     "end_time": "2023-06-07T11:29:44.081810Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:50.774388Z",
     "end_time": "2023-06-07T11:29:50.781270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediction(input):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor(input).view(1, sequence_length, -1)\n",
    "        output = model(input_seq)\n",
    "        return output.squeeze().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:50.970175Z",
     "end_time": "2023-06-07T11:29:50.974005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = prediction(X_test_tensor[-sequence_length:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:30:05.822139Z",
     "end_time": "2023-06-07T11:30:05.837688Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:30:06.606259Z",
     "end_time": "2023-06-07T11:30:06.609147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test_tensor[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:30:07.257100Z",
     "end_time": "2023-06-07T11:30:07.266004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does not exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('master_df.pkl'):\n",
    "    print('does not exist')\n",
    "else:\n",
    "    print('exists')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:52:46.267901Z",
     "end_time": "2023-06-13T10:52:46.274414Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:13:04.741373Z",
     "end_time": "2023-06-07T12:13:04.749586Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autogloun"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "import os.path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.035462Z",
     "end_time": "2023-06-13T11:24:59.037196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "imp_features = list(set([\"day_8_error\", \"gfs-ens-bc_9\", \"ecmwf_diff_8\", \"noon\", \"cmc-ens_9\",\n",
    "                    \"gfs-ens-bc_10\", \"gfs-ens-bc_9\", \"ecmwf_diff_9\", \"cmc-ens_10\",\n",
    "                    \"noon\", \"gfs-ens-bc_10\", \"gfs-ens-bc_11\", \"gfs-ens-bc_9\",\n",
    "                    \"cmc-ens_11\", \"gfs-ens-bc_11\", \"gfs-ens-bc_12\", \"gfs-ens-bc_10\",\n",
    "                    \"gfs-ens-bc_12\", \"gfs-ens-bc_13\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.038531Z",
     "end_time": "2023-06-13T11:24:59.040910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "labels = ['ecmwf-eps_9', 'ecmwf-eps_10', 'ecmwf-eps_11', 'ecmwf-eps_12', 'ecmwf-eps_13',\n",
    "          'ecmwf-eps_14']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.041409Z",
     "end_time": "2023-06-13T11:24:59.043854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "#y = master_df.iloc[:, -6:].copy()\n",
    "#y = y.reset_index(drop=True)\n",
    "#X = master_df.iloc[:, :-6].copy()\n",
    "#X['Date'] = X.index\n",
    "#X = X.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.048878Z",
     "end_time": "2023-06-13T11:24:59.086539Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "#auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "#X = auto_ml_pipeline_feature_generator.fit_transform(X=X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.052726Z",
     "end_time": "2023-06-13T11:24:59.086663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.055040Z",
     "end_time": "2023-06-13T11:24:59.086697Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "#df = pd.concat([X, y], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:24:59.057632Z",
     "end_time": "2023-06-13T11:24:59.086762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "master_df = pd.read_pickle(f'master_df_{degree_days}.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:37:28.982646Z",
     "end_time": "2023-06-13T12:37:28.989510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "train_len = 0.8\n",
    "train_data = TabularDataset(master_df[:int(len(master_df)*train_len)])\n",
    "test_data = TabularDataset(master_df[int(len(master_df)*train_len):])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:37:30.329288Z",
     "end_time": "2023-06-13T12:37:30.342198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "#train_data = train_data.loc[:, imp_features + labels]\n",
    "#test_data = test_data.loc[:, imp_features + labels]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:16:12.622074Z",
     "end_time": "2023-06-13T12:16:12.633900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = 'models'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T09:39:24.353501Z",
     "end_time": "2023-06-13T09:39:24.365059Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(train_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T09:39:24.687366Z",
     "end_time": "2023-06-13T09:39:24.702544Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
    "        if len(labels) < 2:\n",
    "            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
    "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def feature_imp(self,data, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.feature_importance(data, **kwargs)\n",
    "            print(f\"Evaluating feature importance for label: {label} ...\")\n",
    "        return eval_dict\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:05:05.105348Z",
     "end_time": "2023-06-13T10:05:05.136269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multi_predictor = MultilabelPredictor(labels=labels, path=save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:05:10.897133Z",
     "end_time": "2023-06-13T10:05:10.907180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multi_predictor.fit(train_data) # add presets='best_quality' for better results, but longer runtime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:05:11.449709Z",
     "end_time": "2023-06-13T10:06:35.437833Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_imp = multi_predictor.feature_imp(train_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:06:35.810849Z",
     "end_time": "2023-06-13T10:06:50.865292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(feature_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T10:08:33.533764Z",
     "end_time": "2023-06-13T10:08:33.564910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "data": {
      "text/plain": "                     gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  \\\n2022-06-03 00:00:00        -0.324         -0.276         -0.110   \n2022-06-03 12:00:00        -0.233         -0.238         -0.207   \n2022-06-04 00:00:00        -0.164         -0.103         -0.039   \n2022-06-04 12:00:00        -0.224         -0.029         -0.032   \n2022-06-05 00:00:00         0.238          0.247          0.328   \n\n                     gfs-ens-bc_12  gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  \\\n2022-06-03 00:00:00         -0.182         -0.039          0.143      0.272   \n2022-06-03 12:00:00         -0.074          0.055          0.106     -0.070   \n2022-06-04 00:00:00          0.044          0.033          0.025      0.201   \n2022-06-04 12:00:00         -0.087         -0.065          0.020     -0.119   \n2022-06-05 00:00:00          0.372          0.151         -0.013     -0.160   \n\n                     cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  ecmwf_diff_8  \\\n2022-06-03 00:00:00       0.178       0.121       0.060  ...        -0.456   \n2022-06-03 12:00:00      -0.170      -0.144      -0.096  ...        -0.433   \n2022-06-04 00:00:00      -0.113      -0.270      -0.269  ...        -0.194   \n2022-06-04 12:00:00      -0.236      -0.202      -0.056  ...        -0.554   \n2022-06-05 00:00:00      -0.389      -0.462      -0.334  ...         0.011   \n\n                     ecmwf_diff_9  error_9  error_10  error_11  error_12  \\\n2022-06-03 00:00:00        -0.438   -0.122     0.004     0.088     0.016   \n2022-06-03 12:00:00        -0.322   -0.128    -0.068     0.026     0.021   \n2022-06-04 00:00:00         0.044   -0.064    -0.042    -0.085    -0.028   \n2022-06-04 12:00:00        -0.198    0.032     0.000     0.036     0.072   \n2022-06-05 00:00:00        -0.020   -0.189    -0.179    -0.218    -0.228   \n\n                     error_13  error_14  noon  day_8_error  \n2022-06-03 00:00:00    -0.049    -0.072   0.0       -0.137  \n2022-06-03 12:00:00     0.002     0.007   1.0       -0.055  \n2022-06-04 00:00:00     0.021     0.030   0.0        0.034  \n2022-06-04 12:00:00     0.047     0.033   1.0       -0.191  \n2022-06-05 00:00:00    -0.159    -0.049   0.0        0.387  \n\n[5 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>ecmwf_diff_8</th>\n      <th>ecmwf_diff_9</th>\n      <th>error_9</th>\n      <th>error_10</th>\n      <th>error_11</th>\n      <th>error_12</th>\n      <th>error_13</th>\n      <th>error_14</th>\n      <th>noon</th>\n      <th>day_8_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-06-03 00:00:00</th>\n      <td>-0.324</td>\n      <td>-0.276</td>\n      <td>-0.110</td>\n      <td>-0.182</td>\n      <td>-0.039</td>\n      <td>0.143</td>\n      <td>0.272</td>\n      <td>0.178</td>\n      <td>0.121</td>\n      <td>0.060</td>\n      <td>...</td>\n      <td>-0.456</td>\n      <td>-0.438</td>\n      <td>-0.122</td>\n      <td>0.004</td>\n      <td>0.088</td>\n      <td>0.016</td>\n      <td>-0.049</td>\n      <td>-0.072</td>\n      <td>0.0</td>\n      <td>-0.137</td>\n    </tr>\n    <tr>\n      <th>2022-06-03 12:00:00</th>\n      <td>-0.233</td>\n      <td>-0.238</td>\n      <td>-0.207</td>\n      <td>-0.074</td>\n      <td>0.055</td>\n      <td>0.106</td>\n      <td>-0.070</td>\n      <td>-0.170</td>\n      <td>-0.144</td>\n      <td>-0.096</td>\n      <td>...</td>\n      <td>-0.433</td>\n      <td>-0.322</td>\n      <td>-0.128</td>\n      <td>-0.068</td>\n      <td>0.026</td>\n      <td>0.021</td>\n      <td>0.002</td>\n      <td>0.007</td>\n      <td>1.0</td>\n      <td>-0.055</td>\n    </tr>\n    <tr>\n      <th>2022-06-04 00:00:00</th>\n      <td>-0.164</td>\n      <td>-0.103</td>\n      <td>-0.039</td>\n      <td>0.044</td>\n      <td>0.033</td>\n      <td>0.025</td>\n      <td>0.201</td>\n      <td>-0.113</td>\n      <td>-0.270</td>\n      <td>-0.269</td>\n      <td>...</td>\n      <td>-0.194</td>\n      <td>0.044</td>\n      <td>-0.064</td>\n      <td>-0.042</td>\n      <td>-0.085</td>\n      <td>-0.028</td>\n      <td>0.021</td>\n      <td>0.030</td>\n      <td>0.0</td>\n      <td>0.034</td>\n    </tr>\n    <tr>\n      <th>2022-06-04 12:00:00</th>\n      <td>-0.224</td>\n      <td>-0.029</td>\n      <td>-0.032</td>\n      <td>-0.087</td>\n      <td>-0.065</td>\n      <td>0.020</td>\n      <td>-0.119</td>\n      <td>-0.236</td>\n      <td>-0.202</td>\n      <td>-0.056</td>\n      <td>...</td>\n      <td>-0.554</td>\n      <td>-0.198</td>\n      <td>0.032</td>\n      <td>0.000</td>\n      <td>0.036</td>\n      <td>0.072</td>\n      <td>0.047</td>\n      <td>0.033</td>\n      <td>1.0</td>\n      <td>-0.191</td>\n    </tr>\n    <tr>\n      <th>2022-06-05 00:00:00</th>\n      <td>0.238</td>\n      <td>0.247</td>\n      <td>0.328</td>\n      <td>0.372</td>\n      <td>0.151</td>\n      <td>-0.013</td>\n      <td>-0.160</td>\n      <td>-0.389</td>\n      <td>-0.462</td>\n      <td>-0.334</td>\n      <td>...</td>\n      <td>0.011</td>\n      <td>-0.020</td>\n      <td>-0.189</td>\n      <td>-0.179</td>\n      <td>-0.218</td>\n      <td>-0.228</td>\n      <td>-0.159</td>\n      <td>-0.049</td>\n      <td>0.0</td>\n      <td>0.387</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_predictor = MultilabelPredictor.load(f\"models/{degree_days}\")\n",
    "test_data_nolab = test_data.drop(columns=labels)\n",
    "test_data_nolab.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:37:57.498125Z",
     "end_time": "2023-06-13T12:37:57.507321Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with TabularPredictor for label: ecmwf-eps_9 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_10 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_11 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_12 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_13 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_14 ...\n",
      "Predictions:  \n",
      "                      ecmwf-eps_9  ecmwf-eps_10  ecmwf-eps_11  ecmwf-eps_12  \\\n",
      "2022-06-03 00:00:00    -0.112006     -0.088617      0.014164     -0.012020   \n",
      "2022-06-03 12:00:00    -0.150523     -0.161773     -0.138145     -0.039328   \n",
      "2022-06-04 00:00:00     0.024588      0.105082     -0.015106     -0.056605   \n",
      "2022-06-04 12:00:00    -0.235750     -0.113127     -0.080126     -0.065439   \n",
      "2022-06-05 00:00:00     0.339581      0.240902      0.115310      0.095067   \n",
      "...                          ...           ...           ...           ...   \n",
      "2023-05-15 00:00:00     0.892709      0.948564      0.590621      0.485053   \n",
      "2023-05-15 12:00:00    -0.763332     -0.264598      0.051944      0.133531   \n",
      "2023-05-16 00:00:00     0.343141      0.617715      0.948110      0.996496   \n",
      "2023-05-16 12:00:00    -0.620709     -0.524967     -0.347478     -0.122739   \n",
      "2023-05-17 00:00:00     0.484516      0.352268      0.084866     -0.070741   \n",
      "\n",
      "                     ecmwf-eps_13  ecmwf-eps_14  \n",
      "2022-06-03 00:00:00      0.032284      0.099670  \n",
      "2022-06-03 12:00:00      0.015416     -0.037781  \n",
      "2022-06-04 00:00:00     -0.077677     -0.042504  \n",
      "2022-06-04 12:00:00     -0.017443     -0.025403  \n",
      "2022-06-05 00:00:00     -0.009681      0.012132  \n",
      "...                           ...           ...  \n",
      "2023-05-15 00:00:00      0.298398      0.151416  \n",
      "2023-05-15 12:00:00      0.116493      0.068487  \n",
      "2023-05-16 00:00:00      0.892735      0.662063  \n",
      "2023-05-16 12:00:00      0.027350      0.017017  \n",
      "2023-05-17 00:00:00     -0.344825     -0.191301  \n",
      "\n",
      "[697 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions = multi_predictor.predict(test_data_nolab)\n",
    "print(\"Predictions:  \\n\", predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:38:21.235261Z",
     "end_time": "2023-06-13T12:38:21.650638Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "data": {
      "text/plain": "ecmwf-eps_9     0.484516\necmwf-eps_10    0.352268\necmwf-eps_11    0.084866\necmwf-eps_12   -0.070741\necmwf-eps_13   -0.344825\necmwf-eps_14   -0.191301\nName: 2023-05-17 00:00:00, dtype: float32"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.iloc[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:38:25.000796Z",
     "end_time": "2023-06-13T12:38:25.021581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "data": {
      "text/plain": "ecmwf-eps_9     0.482\necmwf-eps_10    0.299\necmwf-eps_11    0.191\necmwf-eps_12   -0.095\necmwf-eps_13   -0.359\necmwf-eps_14   -0.209\nName: 2023-05-17 00:00:00, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_data[labels].iloc[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:38:25.219487Z",
     "end_time": "2023-06-13T12:38:25.228947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.5182784842015763\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5182784842015763,\n",
      "    \"mean_squared_error\": -0.2686125871862836,\n",
      "    \"mean_absolute_error\": -0.29404506450034873,\n",
      "    \"r2\": 0.8910168159449116,\n",
      "    \"pearsonr\": 0.9453960067777752,\n",
      "    \"median_absolute_error\": -0.0983274203538893\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.7586561956686182\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.7586561956686182,\n",
      "    \"mean_squared_error\": -0.5755592232263808,\n",
      "    \"mean_absolute_error\": -0.5057755286397714,\n",
      "    \"r2\": 0.7438467454212415,\n",
      "    \"pearsonr\": 0.8643943879379684,\n",
      "    \"median_absolute_error\": -0.29827007627487134\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.9607805409260292\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9607805409260292,\n",
      "    \"mean_squared_error\": -0.9230992478221133,\n",
      "    \"mean_absolute_error\": -0.6406027082312782,\n",
      "    \"r2\": 0.5040882356919334,\n",
      "    \"pearsonr\": 0.7124773168350835,\n",
      "    \"median_absolute_error\": -0.3891624240875231\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.9986885264530463\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9986885264530463,\n",
      "    \"mean_squared_error\": -0.9973787728689569,\n",
      "    \"mean_absolute_error\": -0.6714587741141933,\n",
      "    \"r2\": 0.3549334392678002,\n",
      "    \"pearsonr\": 0.6054133106216023,\n",
      "    \"median_absolute_error\": -0.4061508388519286\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_10 ...\n",
      "Evaluating TabularPredictor for label: ecmwf-eps_11 ...\n",
      "Evaluating TabularPredictor for label: ecmwf-eps_12 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.9763706484120259\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9763706484120259,\n",
      "    \"mean_squared_error\": -0.95329964308052,\n",
      "    \"mean_absolute_error\": -0.6520909579618103,\n",
      "    \"r2\": 0.2497046201394063,\n",
      "    \"pearsonr\": 0.522621209553458,\n",
      "    \"median_absolute_error\": -0.38203431797027676\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_13 ...\n",
      "Evaluating TabularPredictor for label: ecmwf-eps_14 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -0.9260463709798867\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9260463709798867,\n",
      "    \"mean_squared_error\": -0.857561881205018,\n",
      "    \"mean_absolute_error\": -0.6227739128608017,\n",
      "    \"r2\": 0.21307507786829805,\n",
      "    \"pearsonr\": 0.4791453347637843,\n",
      "    \"median_absolute_error\": -0.41341648054122926\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated using metrics: {'ecmwf-eps_9': root_mean_squared_error, 'ecmwf-eps_10': root_mean_squared_error, 'ecmwf-eps_11': root_mean_squared_error, 'ecmwf-eps_12': root_mean_squared_error, 'ecmwf-eps_13': root_mean_squared_error, 'ecmwf-eps_14': root_mean_squared_error}\n"
     ]
    }
   ],
   "source": [
    "evaluations = multi_predictor.evaluate(test_data)\n",
    "#print(evaluations)\n",
    "print(\"Evaluated using metrics:\", multi_predictor.eval_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:38:27.560596Z",
     "end_time": "2023-06-13T12:38:28.329267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ecmwf-eps_9': {'root_mean_squared_error': -0.5182784842015763,\n  'mean_squared_error': -0.2686125871862836,\n  'mean_absolute_error': -0.29404506450034873,\n  'r2': 0.8910168159449116,\n  'pearsonr': 0.9453960067777752,\n  'median_absolute_error': -0.0983274203538893},\n 'ecmwf-eps_10': {'root_mean_squared_error': -0.7586561956686182,\n  'mean_squared_error': -0.5755592232263808,\n  'mean_absolute_error': -0.5057755286397714,\n  'r2': 0.7438467454212415,\n  'pearsonr': 0.8643943879379684,\n  'median_absolute_error': -0.29827007627487134},\n 'ecmwf-eps_11': {'root_mean_squared_error': -0.9607805409260292,\n  'mean_squared_error': -0.9230992478221133,\n  'mean_absolute_error': -0.6406027082312782,\n  'r2': 0.5040882356919334,\n  'pearsonr': 0.7124773168350835,\n  'median_absolute_error': -0.3891624240875231},\n 'ecmwf-eps_12': {'root_mean_squared_error': -0.9986885264530463,\n  'mean_squared_error': -0.9973787728689569,\n  'mean_absolute_error': -0.6714587741141933,\n  'r2': 0.3549334392678002,\n  'pearsonr': 0.6054133106216023,\n  'median_absolute_error': -0.4061508388519286},\n 'ecmwf-eps_13': {'root_mean_squared_error': -0.9763706484120259,\n  'mean_squared_error': -0.95329964308052,\n  'mean_absolute_error': -0.6520909579618103,\n  'r2': 0.2497046201394063,\n  'pearsonr': 0.522621209553458,\n  'median_absolute_error': -0.38203431797027676},\n 'ecmwf-eps_14': {'root_mean_squared_error': -0.9260463709798867,\n  'mean_squared_error': -0.857561881205018,\n  'mean_absolute_error': -0.6227739128608017,\n  'r2': 0.21307507786829805,\n  'pearsonr': 0.4791453347637843,\n  'median_absolute_error': -0.41341648054122926}}"
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T12:38:32.392244Z",
     "end_time": "2023-06-13T12:38:32.418673Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "{'ecmwf-eps_9': {'root_mean_squared_error': -0.5070563022070824,\n",
    "  'mean_squared_error': -0.25710609360792014,\n",
    "  'mean_absolute_error': -0.2798848657393207,\n",
    "  'r2': 0.8956853027072607,\n",
    "  'pearsonr': 0.9477558158265489,\n",
    "  'median_absolute_error': -0.08431462287902924},\n",
    " 'ecmwf-eps_10': {'root_mean_squared_error': -0.7801987677851059,\n",
    "  'mean_squared_error': -0.6087101172533975,\n",
    "  'mean_absolute_error': -0.5127994778988779,\n",
    "  'r2': 0.7290929041925068,\n",
    "  'pearsonr': 0.8539896408843493,\n",
    "  'median_absolute_error': -0.3007085895538335},\n",
    " 'ecmwf-eps_11': {'root_mean_squared_error': -0.9673184506563108,\n",
    "  'mean_squared_error': -0.9357049849801256,\n",
    "  'mean_absolute_error': -0.6515933946707977,\n",
    "  'r2': 0.4973161216758266,\n",
    "  'pearsonr': 0.7102926086311597,\n",
    "  'median_absolute_error': -0.3967944383621216},\n",
    " 'ecmwf-eps_12': {'root_mean_squared_error': -0.9975580503693926,\n",
    "  'mean_squared_error': -0.9951220638567836,\n",
    "  'mean_absolute_error': -0.6839024679556679,\n",
    "  'r2': 0.35639299260967516,\n",
    "  'pearsonr': 0.6123533275180042,\n",
    "  'median_absolute_error': -0.3845637946128857},\n",
    " 'ecmwf-eps_13': {'root_mean_squared_error': -0.9652024686896034,\n",
    "  'mean_squared_error': -0.9316158055645049,\n",
    "  'mean_absolute_error': -0.6578454494274173,\n",
    "  'r2': 0.2667709048317418,\n",
    "  'pearsonr': 0.5357525489886675,\n",
    "  'median_absolute_error': -0.3887484552860272},\n",
    " 'ecmwf-eps_14': {'root_mean_squared_error': -0.9526157480747444,\n",
    "  'mean_squared_error': -0.9074767634800048,\n",
    "  'mean_absolute_error': -0.6393496713255836,\n",
    "  'r2': 0.1672716604026533,\n",
    "  'pearsonr': 0.4348277794095413,\n",
    "  'median_absolute_error': -0.42026434469222984}}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor_class = multi_predictor.get_predictor('ecmwf-eps_9')\n",
    "display(predictor_class.leaderboard(silent=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T09:33:29.081411Z",
     "end_time": "2023-06-13T09:33:29.093110Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# feature importance / training models for individual days"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:27:30.239642Z",
     "end_time": "2023-06-13T11:27:30.252467Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:34:42.733673Z",
     "end_time": "2023-06-13T11:34:42.748580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230613_174328/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230613_174328/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 23\n",
      "Label Column: ecmwf-eps_9\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.132000000000005, -7.687000000000005, 0.01289, 1.61633)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1446.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.51 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 23 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 22 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['noon']\n",
      "\t0.0s = Fit runtime\n",
      "\t23 features in original data used to generate 23 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.9151\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.9127\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.6731\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.6324\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.6461\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.6006\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.6638\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6471\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.5833\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 19.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230613_174328/\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.5138114788406662\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5138114788406662,\n",
      "    \"mean_squared_error\": -0.2640022357884324,\n",
      "    \"mean_absolute_error\": -0.2996831988362279,\n",
      "    \"r2\": 0.8928873566377876,\n",
      "    \"pearsonr\": 0.9460458924510404,\n",
      "    \"median_absolute_error\": -0.1208815002441419\n",
      "}\n",
      "Computing feature importance via permutation shuffling for 23 features using 697 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for ecmwf-eps_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t17.76s\t= Expected runtime (3.55s per shuffle set)\n",
      "\t2.08s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230613_174349/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230613_174349/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 24\n",
      "Label Column: ecmwf-eps_10\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (6.731999999999999, -7.844000000000001, 0.01838, 1.49726)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1436.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.53 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 24 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 23 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['noon']\n",
      "\t0.0s = Fit runtime\n",
      "\t24 features in original data used to generate 24 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.0446\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.0418\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.8867\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.9s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7738\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.8766\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.7075\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.835\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.722\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6905\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 14.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230613_174349/\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.6482294046403163\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6482294046403163,\n",
      "    \"mean_squared_error\": -0.4202013610403389,\n",
      "    \"mean_absolute_error\": -0.42385628521797164,\n",
      "    \"r2\": 0.8129889299566119,\n",
      "    \"pearsonr\": 0.903525858483189,\n",
      "    \"median_absolute_error\": -0.2327110500335685\n",
      "}\n",
      "Computing feature importance via permutation shuffling for 24 features using 697 rows with 5 shuffle sets...\n",
      "\t4.87s\t= Expected runtime (0.97s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for ecmwf-eps_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.44s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230613_174406/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230613_174406/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 25\n",
      "Label Column: ecmwf-eps_11\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.802, -7.863000000000003, 0.02167, 1.37127)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1443.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 24 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['noon']\n",
      "\t0.0s = Fit runtime\n",
      "\t25 features in original data used to generate 25 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.1088\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.1049\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.8489\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7715\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.8624\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.6388\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.8373\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6874\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6357\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.82s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230613_174406/\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.6051291596959765\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6051291596959765,\n",
      "    \"mean_squared_error\": -0.36618129991435866,\n",
      "    \"mean_absolute_error\": -0.4082784703162028,\n",
      "    \"r2\": 0.8032783420357146,\n",
      "    \"pearsonr\": 0.8970626594618069,\n",
      "    \"median_absolute_error\": -0.23859715270996062\n",
      "}\n",
      "Computing feature importance via permutation shuffling for 25 features using 697 rows with 5 shuffle sets...\n",
      "\t9.57s\t= Expected runtime (1.91s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for ecmwf-eps_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.59s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230613_174423/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230613_174423/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 26\n",
      "Label Column: ecmwf-eps_12\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5.904999999999994, -6.439, 0.02074, 1.25962)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1440.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 26 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 25 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['noon']\n",
      "\t0.0s = Fit runtime\n",
      "\t26 features in original data used to generate 26 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.0588\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.0555\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.7845\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7165\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.7678\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.6227\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.7461\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6551\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6192\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 16.12s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230613_174423/\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.5610437632714615\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5610437632714615,\n",
      "    \"mean_squared_error\": -0.3147701043058038,\n",
      "    \"mean_absolute_error\": -0.3568915304182421,\n",
      "    \"r2\": 0.7964186985634407,\n",
      "    \"pearsonr\": 0.8931503694054962,\n",
      "    \"median_absolute_error\": -0.20116862773895505\n",
      "}\n",
      "Computing feature importance via permutation shuffling for 26 features using 697 rows with 5 shuffle sets...\n",
      "\t4.86s\t= Expected runtime (0.97s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for ecmwf-eps_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.56s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230613_174441/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230613_174441/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 27\n",
      "Label Column: ecmwf-eps_13\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5.749000000000002, -5.777999999999995, 0.01271, 1.17099)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1445.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 27 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 26 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['noon']\n",
      "\t0.0s = Fit runtime\n",
      "\t27 features in original data used to generate 27 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.9857\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.9825\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.7137\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.32s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.6134\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.6993\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.5548\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.6665\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.5894\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.88s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.5495\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 16.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230613_174441/\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4980226896564966\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4980226896564966,\n",
      "    \"mean_squared_error\": -0.24802659941269112,\n",
      "    \"mean_absolute_error\": -0.3246777100375008,\n",
      "    \"r2\": 0.8047904318724705,\n",
      "    \"pearsonr\": 0.8986036857267594,\n",
      "    \"median_absolute_error\": -0.1946098747253444\n",
      "}\n",
      "Computing feature importance via permutation shuffling for 27 features using 697 rows with 5 shuffle sets...\n",
      "\t3.58s\t= Expected runtime (0.72s per shuffle set)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for ecmwf-eps_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.2s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "most_important = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    day = i\n",
    "    train_df = train_data.iloc[:, :-5+day]\n",
    "    test_df = test_data.iloc[:, :-5+day]\n",
    "    #display(train_df.head())\n",
    "    label = f\"ecmwf-eps_{day+9}\"\n",
    "    predictor = TabularPredictor(label=label).fit(train_df)\n",
    "    y_test = test_df[label]\n",
    "    test_data_nolab = test_df.drop(columns=[label])\n",
    "    print(f\"results for {label}\")\n",
    "    y_pred = predictor.predict(test_data_nolab)\n",
    "    perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
    "    shap_values = predictor.feature_importance(test_df)\n",
    "    most_important.append(list(shap_values.index[:6].values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:43:28.058427Z",
     "end_time": "2023-06-13T11:44:59.166076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_8_error: 1\n",
      "gfs-ens-bc_9: 3\n",
      "ecmwf_diff_8: 1\n",
      "noon: 2\n",
      "cmc-ens_9: 1\n",
      "gfs-ens-bc_11: 3\n",
      "ecmwf-eps_9: 3\n",
      "gfs-ens-bc_10: 3\n",
      "ecmwf_diff_9: 1\n",
      "cmc-ens_10: 1\n",
      "ecmwf-eps_10: 3\n",
      "cmc-ens_11: 2\n",
      "ecmwf-eps_11: 2\n",
      "gfs-ens-bc_12: 2\n",
      "ecmwf-eps_12: 1\n",
      "gfs-ens-bc_13: 1\n"
     ]
    }
   ],
   "source": [
    "item_counts = Counter([item for sublist in most_important for item in sublist])\n",
    "important_features = []\n",
    "for item, count in item_counts.items():\n",
    "    print(f'{item}: {count}')\n",
    "    important_features.append(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:45:55.654078Z",
     "end_time": "2023-06-13T11:45:55.669126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:47:03.474648Z",
     "end_time": "2023-06-13T11:47:03.482576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:44:59.175659Z",
     "end_time": "2023-06-13T11:44:59.177676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:44:59.177938Z",
     "end_time": "2023-06-13T11:44:59.179691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:44:59.180554Z",
     "end_time": "2023-06-13T11:44:59.182890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:44:59.183514Z",
     "end_time": "2023-06-13T11:44:59.200745Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:35:37.542057Z",
     "end_time": "2023-06-13T11:35:37.544819Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:36:47.084592Z",
     "end_time": "2023-06-13T11:36:47.091913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:37:13.552184Z",
     "end_time": "2023-06-13T11:37:13.564947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T11:44:59.187220Z",
     "end_time": "2023-06-13T11:44:59.201100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### most important features\n",
    "ecmwf-eps_9:\n",
    "- day_8_error\n",
    "- gfs-ens-bc_9\n",
    "- ecmwf_diff_8\n",
    "- noon\n",
    "- cmc-ens_9\n",
    "\n",
    "ecmwf-eps_10:\n",
    "- ecmwf-eps_9\n",
    "- gfs-ens-bc_10\n",
    "- gfs-ens-bc_9\n",
    "- ecmwf_diff_9\n",
    "- cmc-ens_10\n",
    "- noon\n",
    "\n",
    "ecmwf-eps_11:\n",
    "- ecmwf-eps_10\n",
    "- gfs-ens-bc_10\n",
    "- gfs-ens-bc_11\n",
    "- ecmwf-eps_9\n",
    "- gfs-ens-bc_9\n",
    "- cmc-ens_11\n",
    "\n",
    "ecmwf-eps_12:\n",
    "- ecmwf-eps_11\n",
    "- gfs-ens-bc_11\n",
    "- gfs-ens-bc_12\n",
    "- ecmwf-eps_10\n",
    "- gfs-ens-bc_10\n",
    "\n",
    "ecmwf-eps_13:\n",
    "- ecmwf-eps_12\n",
    "- gfs-ens-bc_12\n",
    "- ecmwf-eps_11\n",
    "- gfs-ens-bc_13\n",
    "- ecmwf-eps_10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
