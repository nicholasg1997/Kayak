{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:16.746400Z",
     "end_time": "2023-06-06T13:37:16.752349Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "from meteostat import Stations, Daily\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "path = \"RawData\"\n",
    "\n",
    "\n",
    "def extract_date_time(filename):\n",
    "    \"\"\"\n",
    "    extract the date and time from the filename\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parts = filename.split('.')\n",
    "    date = parts[1]\n",
    "    time = parts[2]\n",
    "    return date, time\n",
    "\n",
    "\n",
    "def get_date(df, file):\n",
    "    \"\"\"get the date from the dataframe and the time from the filename and combine them into a datetime object\n",
    "    :param df: dataframe containing the date\n",
    "    :param file: filename containing the time\n",
    "    :return: datetime object\n",
    "    \"\"\"\n",
    "    #date_str = df[df.iloc[:, 2] == 1].iloc[0]['Date']\n",
    "    date_str = str(file.split('.')[1])\n",
    "    time_str = str(file.split('.')[2])\n",
    "    #date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    date = datetime.strptime(date_str, '%Y%m%d')\n",
    "    time_value = time(int(time_str), 0)\n",
    "    combined_datetime = datetime.combine(date.date(), time_value)\n",
    "    return combined_datetime\n",
    "\n",
    "\n",
    "degree_days = 'gw_hdd'\n",
    "ecmwf_files = glob.glob(path + f'/ecmwf.*.[01][02].{degree_days}.csv')\n",
    "ecmwf_sorted_files = sorted(ecmwf_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[3:]\n",
    "\n",
    "ecmwf_eps_files = glob.glob(path + f'/ecmwf-eps.*.[01][02].{degree_days}.csv')\n",
    "ecmwf_eps_sorted_files = sorted(ecmwf_eps_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "\n",
    "gfs_ens_bc_files = glob.glob(path + f'/gfs-ens-bc.*.[01][02].{degree_days}.csv')\n",
    "gfs_ens_bc_sorted_files = sorted(gfs_ens_bc_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "\n",
    "cmc_ens_files = glob.glob(path + f'/cmc-ens.*.[01][02].{degree_days}.csv')\n",
    "cmc_ens_sorted_files = sorted(cmc_ens_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "for _ in range(2):\n",
    "    set1 = set((extract_date_time(filename) for filename in ecmwf_sorted_files))\n",
    "    set2 = set((extract_date_time(filename) for filename in ecmwf_eps_sorted_files))\n",
    "\n",
    "    ecmwf_sorted_files = [filename for filename in ecmwf_sorted_files if extract_date_time(filename) in set2]\n",
    "    ecmwf_eps_sorted_files = [filename for filename in ecmwf_eps_sorted_files if extract_date_time(filename) in set1]\n",
    "    cmc_ens_sorted_files = [filename for filename in cmc_ens_sorted_files if extract_date_time(filename) in set1]\n",
    "\n",
    "    master_set = set((extract_date_time(filename) for filename in cmc_ens_sorted_files))\n",
    "    gfs_ens_bc_sorted_files = [filename for filename in gfs_ens_bc_sorted_files if\n",
    "                               extract_date_time(filename) in master_set]\n",
    "\n",
    "    master_set = set((extract_date_time(filename) for filename in gfs_ens_bc_sorted_files))\n",
    "\n",
    "    ecmwf_sorted_files = [filename for filename in ecmwf_sorted_files if extract_date_time(filename) in master_set]\n",
    "    ecmwf_eps_sorted_files = [filename for filename in ecmwf_eps_sorted_files if\n",
    "                              extract_date_time(filename) in master_set]\n",
    "    gfs_ens_bc_sorted_files = [filename for filename in gfs_ens_bc_sorted_files if\n",
    "                               extract_date_time(filename) in master_set]\n",
    "    cmc_ens_sorted_files = [filename for filename in cmc_ens_sorted_files if extract_date_time(filename) in master_set]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:16.898578Z",
     "end_time": "2023-06-06T13:37:17.390331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "ecmwf_eps_change_df = pd.DataFrame(columns=['ecmwf-eps_9', 'ecmwf-eps_10', 'ecmwf-eps_11', 'ecmwf-eps_12',\n",
    "                                  'ecmwf-eps_13', 'ecmwf-eps_14'])\n",
    "\n",
    "for i in range(1, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i])\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    for day in range(8, 14):\n",
    "        changes.append(ecmwf_eps_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "    new_row = pd.DataFrame([changes], columns=ecmwf_eps_change_df.columns, index=[date])\n",
    "    ecmwf_eps_change_df = pd.concat([ecmwf_eps_change_df, new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:17.394320Z",
     "end_time": "2023-06-06T13:37:25.171868Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "ecmwf_change_df = pd.DataFrame(columns=['ecmwf_diff_8', 'ecmwf_diff_9',])\n",
    "for i in range(1, len(ecmwf_sorted_files)):\n",
    "    ecmwf_df = pd.read_csv(ecmwf_sorted_files[i])\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1]) #one day behind\n",
    "\n",
    "    ecmwf = ecmwf_df.iloc[8]\n",
    "    ecmwf_eps = ecmwf_eps_df.iloc[9]\n",
    "\n",
    "    date = get_date(ecmwf_df, ecmwf_sorted_files[i])\n",
    "    prev_date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    for day in range(8,10):\n",
    "        changes.append(ecmwf_df.iloc[day - offset]['Value'] - ecmwf_eps_df.iloc[day]['Value'])\n",
    "    new_row = pd.DataFrame([changes], columns=ecmwf_change_df.columns, index=[date])\n",
    "    ecmwf_change_df = pd.concat([ecmwf_change_df, new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:25.175325Z",
     "end_time": "2023-06-06T13:37:32.038846Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "gfs_ens_bc_change_df = pd.DataFrame(columns=['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12',\n",
    "                                  'gfs-ens-bc_13', 'gfs-ens-bc_14'])\n",
    "\n",
    "for i in range(1, len(gfs_ens_bc_sorted_files)):\n",
    "    gfs_ens_bc_df = pd.read_csv(gfs_ens_bc_sorted_files[i])\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "\n",
    "    date = get_date(gfs_ens_bc_df, gfs_ens_bc_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    for day in range(8, 14):\n",
    "        changes.append(gfs_ens_bc_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "    new_row = pd.DataFrame([changes], columns=gfs_ens_bc_change_df.columns, index=[date])\n",
    "    gfs_ens_bc_change_df = pd.concat([gfs_ens_bc_change_df, new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:32.043597Z",
     "end_time": "2023-06-06T13:37:40.032081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "cmc_ens_change_df = pd.DataFrame(columns=['cmc-ens_9', 'cmc-ens_10', 'cmc-ens_11', 'cmc-ens_12',\n",
    "                                  'cmc-ens_13', 'cmc-ens_14'])\n",
    "\n",
    "for i in range(1, len(cmc_ens_sorted_files)):\n",
    "    cmc_ens_df = pd.read_csv(cmc_ens_sorted_files[i])\n",
    "    gfs_ens_bc_df = pd.read_csv(gfs_ens_bc_sorted_files[i])\n",
    "    date = get_date(cmc_ens_df, cmc_ens_sorted_files[i])\n",
    "\n",
    "    changes = []\n",
    "    for day in range(8, 14):\n",
    "        changes.append(cmc_ens_df.iloc[day]['Value'] - gfs_ens_bc_df.iloc[day]['Value'])\n",
    "    new_row = pd.DataFrame([changes], columns=cmc_ens_change_df.columns, index=[date])\n",
    "    cmc_ens_change_df = pd.concat([cmc_ens_change_df, new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:40.035009Z",
     "end_time": "2023-06-06T13:37:47.988649Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "day_8_error = pd.DataFrame(columns=['day_8_error'])\n",
    "for i in range(1, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i])\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "    day = 7\n",
    "    changes = []\n",
    "    changes.append(ecmwf_eps_df.iloc[day]['Value'] - prev_ecmwf_eps_df.iloc[day + offset]['Value'])\n",
    "    new_row = pd.DataFrame([changes], columns=day_8_error.columns, index=[date])\n",
    "    day_8_error = pd.concat([day_8_error, new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:47.989483Z",
     "end_time": "2023-06-06T13:37:53.912402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "errors_df = pd.DataFrame(columns=['error_9', 'error_10', 'error_11', 'error_12', 'error_13', 'error_14'])\n",
    "\n",
    "for i in range(2, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-2])\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    errors = []\n",
    "    for day in range(8, 14):\n",
    "        errors.append(ecmwf_eps_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "    new_row = pd.DataFrame([errors], columns=errors_df.columns, index=[date])\n",
    "    errors_df = pd.concat([errors_df, new_row])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:37:53.916202Z",
     "end_time": "2023-06-06T13:38:01.353988Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                     gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  \\\n2018-07-11 00:00:00         0.009          0.006          0.002   \n2018-07-11 12:00:00         0.004          0.001          0.000   \n2018-07-12 00:00:00         0.003          0.002          0.002   \n2018-07-12 12:00:00         0.001          0.002          0.004   \n2018-07-13 00:00:00         0.001          0.002          0.002   \n\n                     gfs-ens-bc_12  gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  \\\n2018-07-11 00:00:00          0.001          0.013          0.013     -0.006   \n2018-07-11 12:00:00          0.005          0.008          0.018     -0.003   \n2018-07-12 00:00:00          0.006          0.009          0.020     -0.004   \n2018-07-12 12:00:00          0.009          0.021          0.015     -0.002   \n2018-07-13 00:00:00          0.007          0.019          0.021     -0.001   \n\n                     cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  error_12  \\\n2018-07-11 00:00:00      -0.003      -0.001      -0.013  ...     0.000   \n2018-07-11 12:00:00      -0.002      -0.001      -0.005  ...    -0.002   \n2018-07-12 00:00:00      -0.002      -0.006      -0.009  ...     0.001   \n2018-07-12 12:00:00      -0.002      -0.004      -0.008  ...     0.001   \n2018-07-13 00:00:00      -0.003      -0.008      -0.022  ...     0.000   \n\n                     error_13  error_14  day_8_error  ecmwf-eps_9  \\\n2018-07-11 00:00:00     0.000     0.000        0.001        0.001   \n2018-07-11 12:00:00     0.002    -0.002        0.005        0.001   \n2018-07-12 00:00:00    -0.003     0.001       -0.001       -0.001   \n2018-07-12 12:00:00    -0.001     0.003        0.003        0.000   \n2018-07-13 00:00:00     0.001     0.004        0.000        0.000   \n\n                     ecmwf-eps_10  ecmwf-eps_11  ecmwf-eps_12  ecmwf-eps_13  \\\n2018-07-11 00:00:00        -0.001         0.000         0.000         0.000   \n2018-07-11 12:00:00         0.000        -0.001        -0.001        -0.001   \n2018-07-12 00:00:00        -0.001         0.000         0.000        -0.001   \n2018-07-12 12:00:00        -0.001         0.001         0.001         0.006   \n2018-07-13 00:00:00        -0.001        -0.001        -0.002         0.000   \n\n                     ecmwf-eps_14  \n2018-07-11 00:00:00         0.000  \n2018-07-11 12:00:00        -0.001  \n2018-07-12 00:00:00         0.001  \n2018-07-12 12:00:00         0.003  \n2018-07-13 00:00:00        -0.001  \n\n[5 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>error_12</th>\n      <th>error_13</th>\n      <th>error_14</th>\n      <th>day_8_error</th>\n      <th>ecmwf-eps_9</th>\n      <th>ecmwf-eps_10</th>\n      <th>ecmwf-eps_11</th>\n      <th>ecmwf-eps_12</th>\n      <th>ecmwf-eps_13</th>\n      <th>ecmwf-eps_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-07-11 00:00:00</th>\n      <td>0.009</td>\n      <td>0.006</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.013</td>\n      <td>0.013</td>\n      <td>-0.006</td>\n      <td>-0.003</td>\n      <td>-0.001</td>\n      <td>-0.013</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2018-07-11 12:00:00</th>\n      <td>0.004</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.008</td>\n      <td>0.018</td>\n      <td>-0.003</td>\n      <td>-0.002</td>\n      <td>-0.001</td>\n      <td>-0.005</td>\n      <td>...</td>\n      <td>-0.002</td>\n      <td>0.002</td>\n      <td>-0.002</td>\n      <td>0.005</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n    </tr>\n    <tr>\n      <th>2018-07-12 00:00:00</th>\n      <td>0.003</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.006</td>\n      <td>0.009</td>\n      <td>0.020</td>\n      <td>-0.004</td>\n      <td>-0.002</td>\n      <td>-0.006</td>\n      <td>-0.009</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>-0.003</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>2018-07-12 12:00:00</th>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.004</td>\n      <td>0.009</td>\n      <td>0.021</td>\n      <td>0.015</td>\n      <td>-0.002</td>\n      <td>-0.002</td>\n      <td>-0.004</td>\n      <td>-0.008</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.006</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>2018-07-13 00:00:00</th>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.007</td>\n      <td>0.019</td>\n      <td>0.021</td>\n      <td>-0.001</td>\n      <td>-0.003</td>\n      <td>-0.008</td>\n      <td>-0.022</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.002</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_df = pd.concat([gfs_ens_bc_change_df, cmc_ens_change_df, ecmwf_change_df, errors_df, day_8_error, ecmwf_eps_change_df], axis=1)\n",
    "master_df.fillna(0, inplace=True)\n",
    "display(master_df[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.356333Z",
     "end_time": "2023-06-06T13:38:01.373198Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = master_df.iloc[:, :-6]\n",
    "y = master_df.iloc[:, -6:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:48:55.664139Z",
     "end_time": "2023-06-06T11:48:55.667857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:48:55.667563Z",
     "end_time": "2023-06-06T11:48:55.696374Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:48:55.680203Z",
     "end_time": "2023-06-06T11:49:59.598339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:49:59.601113Z",
     "end_time": "2023-06-06T11:49:59.801538Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_features = master_df.iloc[:, :-6].values ** 2\n",
    "target_variables = master_df.iloc[:, -6:].values\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_features, target_variables, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale the input features based on the training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test data based on the training data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data to PyTorch tensors\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train_scaled)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "X_val_tensor = torch.Tensor(X_val_scaled)\n",
    "y_val_tensor = torch.Tensor(y_val)\n",
    "X_test_tensor = torch.Tensor(X_test_scaled)\n",
    "y_test_tensor = torch.Tensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:49:59.805073Z",
     "end_time": "2023-06-06T11:49:59.811015Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#benchmark error\n",
    "total_mse = 0\n",
    "c = 0\n",
    "for i in range(1, len(y_test_tensor)):\n",
    "    #mse = mean_squared_error(y_test_tensor[i], y_test_tensor[i-1])\n",
    "    mse = mean_squared_error(y_test_tensor[i], [0,0,0,0,0,0])\n",
    "    total_mse += mse\n",
    "    c += 1\n",
    "\n",
    "total_mse/c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:49:59.815325Z",
     "end_time": "2023-06-06T11:49:59.897205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, output_size)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = F.relu(out)  # Apply ReLU activation between LSTM and first dense layer\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = F.relu(out)  # Apply ReLU activation to the output of the first dense layer\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:49:59.896628Z",
     "end_time": "2023-06-06T11:49:59.904045Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = y_train_tensor.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "lr = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:49:59.906461Z",
     "end_time": "2023-06-06T11:49:59.908483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T11:49:59.912922Z",
     "end_time": "2023-06-06T11:49:59.933003Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "sequence_length = 5  # Number of previous days to consider\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loop through each sequence in the training data\n",
    "    for i in range(sequence_length, X_train_tensor.shape[0]):\n",
    "        # Extract the current sequence and target\n",
    "        input_seq = X_train_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "        target_seq = y_train_tensor[i]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / (X_train_tensor.shape[0] - sequence_length)\n",
    "    train_losses.append(average_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {average_loss}')\n",
    "\n",
    "    # Validation stage\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for i in range(sequence_length, X_val_tensor.shape[0]):\n",
    "            input_seq = X_val_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "            target_seq = y_val_tensor[i]\n",
    "\n",
    "            output = model(input_seq)\n",
    "            val_loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / (X_val_tensor.shape[0] - sequence_length)\n",
    "        val_losses.append(average_val_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {average_val_loss}')\n",
    "\n",
    "        # Check if current model is the best based on validation loss\n",
    "        if average_val_loss < best_loss:\n",
    "            best_loss = average_val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "# After training, use the best model for testing\n",
    "model = best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:50:07.386236Z",
     "end_time": "2023-06-01T12:57:09.192598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(train_losses, label='Training Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "plot_losses(train_losses, val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:57:09.219917Z",
     "end_time": "2023-06-01T12:57:09.330564Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(sequence_length, X_test_tensor.shape[0]):\n",
    "        input_seq = X_test_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "        target_seq = y_test_tensor[i]\n",
    "\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Extract the scalar value from the tensor and append it to predictions\n",
    "        predictions.append(output.squeeze().tolist())\n",
    "\n",
    "    average_test_loss = test_loss / (X_test_tensor.shape[0] - sequence_length)\n",
    "    print(f'Test Loss: {average_test_loss}')\n",
    "\n",
    "    # Convert the predictions and target values to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    targets = y_test_tensor[sequence_length:].numpy()\n",
    "\n",
    "    # Evaluate the performance using appropriate metrics\n",
    "    # For example, calculate mean squared error (MSE)\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    print(f'Mean Squared Error (MSE): {mse}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:57:09.332633Z",
     "end_time": "2023-06-01T12:57:09.760812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediction(input):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor(input).view(1, sequence_length, -1)\n",
    "        output = model(input_seq)\n",
    "        return output.squeeze().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:57:09.763079Z",
     "end_time": "2023-06-01T12:57:09.765094Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = prediction(X_test_tensor[-5:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:57:09.766375Z",
     "end_time": "2023-06-01T12:57:09.769971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:57:09.771177Z",
     "end_time": "2023-06-01T12:57:09.774145Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test_tensor[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-01T12:57:09.775978Z",
     "end_time": "2023-06-01T12:57:09.796432Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autogloun"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.373848Z",
     "end_time": "2023-06-06T13:38:01.380483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_len = 0.8\n",
    "train_data = TabularDataset(master_df[:int(len(master_df)*train_len)])\n",
    "test_data = TabularDataset(master_df[int(len(master_df)*train_len):])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.380012Z",
     "end_time": "2023-06-06T13:38:01.397186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "labels = ['ecmwf-eps_9', 'ecmwf-eps_10', 'ecmwf-eps_11', 'ecmwf-eps_12', 'ecmwf-eps_13',\n",
    "          'ecmwf-eps_14']\n",
    "save_path = 'models'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.382701Z",
     "end_time": "2023-06-06T13:38:01.402743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                     gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  \\\n2018-07-11 00:00:00         0.009          0.006          0.002   \n2018-07-11 12:00:00         0.004          0.001          0.000   \n2018-07-12 00:00:00         0.003          0.002          0.002   \n2018-07-12 12:00:00         0.001          0.002          0.004   \n2018-07-13 00:00:00         0.001          0.002          0.002   \n\n                     gfs-ens-bc_12  gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  \\\n2018-07-11 00:00:00          0.001          0.013          0.013     -0.006   \n2018-07-11 12:00:00          0.005          0.008          0.018     -0.003   \n2018-07-12 00:00:00          0.006          0.009          0.020     -0.004   \n2018-07-12 12:00:00          0.009          0.021          0.015     -0.002   \n2018-07-13 00:00:00          0.007          0.019          0.021     -0.001   \n\n                     cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  error_12  \\\n2018-07-11 00:00:00      -0.003      -0.001      -0.013  ...     0.000   \n2018-07-11 12:00:00      -0.002      -0.001      -0.005  ...    -0.002   \n2018-07-12 00:00:00      -0.002      -0.006      -0.009  ...     0.001   \n2018-07-12 12:00:00      -0.002      -0.004      -0.008  ...     0.001   \n2018-07-13 00:00:00      -0.003      -0.008      -0.022  ...     0.000   \n\n                     error_13  error_14  day_8_error  ecmwf-eps_9  \\\n2018-07-11 00:00:00     0.000     0.000        0.001        0.001   \n2018-07-11 12:00:00     0.002    -0.002        0.005        0.001   \n2018-07-12 00:00:00    -0.003     0.001       -0.001       -0.001   \n2018-07-12 12:00:00    -0.001     0.003        0.003        0.000   \n2018-07-13 00:00:00     0.001     0.004        0.000        0.000   \n\n                     ecmwf-eps_10  ecmwf-eps_11  ecmwf-eps_12  ecmwf-eps_13  \\\n2018-07-11 00:00:00        -0.001         0.000         0.000         0.000   \n2018-07-11 12:00:00         0.000        -0.001        -0.001        -0.001   \n2018-07-12 00:00:00        -0.001         0.000         0.000        -0.001   \n2018-07-12 12:00:00        -0.001         0.001         0.001         0.006   \n2018-07-13 00:00:00        -0.001        -0.001        -0.002         0.000   \n\n                     ecmwf-eps_14  \n2018-07-11 00:00:00         0.000  \n2018-07-11 12:00:00        -0.001  \n2018-07-12 00:00:00         0.001  \n2018-07-12 12:00:00         0.003  \n2018-07-13 00:00:00        -0.001  \n\n[5 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>error_12</th>\n      <th>error_13</th>\n      <th>error_14</th>\n      <th>day_8_error</th>\n      <th>ecmwf-eps_9</th>\n      <th>ecmwf-eps_10</th>\n      <th>ecmwf-eps_11</th>\n      <th>ecmwf-eps_12</th>\n      <th>ecmwf-eps_13</th>\n      <th>ecmwf-eps_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-07-11 00:00:00</th>\n      <td>0.009</td>\n      <td>0.006</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.013</td>\n      <td>0.013</td>\n      <td>-0.006</td>\n      <td>-0.003</td>\n      <td>-0.001</td>\n      <td>-0.013</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2018-07-11 12:00:00</th>\n      <td>0.004</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.005</td>\n      <td>0.008</td>\n      <td>0.018</td>\n      <td>-0.003</td>\n      <td>-0.002</td>\n      <td>-0.001</td>\n      <td>-0.005</td>\n      <td>...</td>\n      <td>-0.002</td>\n      <td>0.002</td>\n      <td>-0.002</td>\n      <td>0.005</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n    </tr>\n    <tr>\n      <th>2018-07-12 00:00:00</th>\n      <td>0.003</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.006</td>\n      <td>0.009</td>\n      <td>0.020</td>\n      <td>-0.004</td>\n      <td>-0.002</td>\n      <td>-0.006</td>\n      <td>-0.009</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>-0.003</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>2018-07-12 12:00:00</th>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.004</td>\n      <td>0.009</td>\n      <td>0.021</td>\n      <td>0.015</td>\n      <td>-0.002</td>\n      <td>-0.002</td>\n      <td>-0.004</td>\n      <td>-0.008</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>0.003</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.006</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>2018-07-13 00:00:00</th>\n      <td>0.001</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.007</td>\n      <td>0.019</td>\n      <td>0.021</td>\n      <td>-0.001</td>\n      <td>-0.003</td>\n      <td>-0.008</td>\n      <td>-0.022</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.004</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>-0.001</td>\n      <td>-0.002</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.395762Z",
     "end_time": "2023-06-06T13:38:01.421946Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
    "        if len(labels) < 2:\n",
    "            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
    "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.398542Z",
     "end_time": "2023-06-06T13:38:01.434531Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_9\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_10\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_11\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_12\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_13\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_14\"\n"
     ]
    }
   ],
   "source": [
    "multi_predictor = MultilabelPredictor(labels=labels, path=save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.416837Z",
     "end_time": "2023-06-06T13:38:01.435502Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multi_predictor.fit(train_data, presets='best_quality')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T12:16:10.923596Z",
     "end_time": "2023-06-06T12:30:48.077668Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                     gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  \\\n2022-06-03 00:00:00        -0.056          0.284          0.235   \n2022-06-03 12:00:00         0.245          0.149          0.015   \n2022-06-04 00:00:00         0.091          0.101         -0.160   \n2022-06-04 12:00:00         0.102         -0.112         -0.176   \n2022-06-05 00:00:00         0.016         -0.119         -0.257   \n\n                     gfs-ens-bc_12  gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  \\\n2022-06-03 00:00:00         -0.012         -0.186         -0.254     -0.062   \n2022-06-03 12:00:00         -0.177         -0.229         -0.307      0.023   \n2022-06-04 00:00:00         -0.319         -0.295         -0.112     -0.053   \n2022-06-04 12:00:00         -0.194         -0.116         -0.056     -0.052   \n2022-06-05 00:00:00         -0.154         -0.074          0.054      0.053   \n\n                     cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  cmc-ens_14  \\\n2022-06-03 00:00:00      -0.088      -0.044       0.043  ...       0.115   \n2022-06-03 12:00:00      -0.067      -0.048       0.265  ...       0.162   \n2022-06-04 00:00:00       0.052       0.277       0.186  ...      -0.024   \n2022-06-04 12:00:00       0.000       0.228       0.071  ...      -0.171   \n2022-06-05 00:00:00       0.370       0.110      -0.109  ...       0.240   \n\n                     ecmwf_diff_8  ecmwf_diff_9  error_9  error_10  error_11  \\\n2022-06-03 00:00:00        -0.089        -0.174   -0.140    -0.468     0.275   \n2022-06-03 12:00:00        -0.023        -0.057    0.297    -0.509     0.013   \n2022-06-04 00:00:00        -0.017        -0.042   -0.233     0.392    -0.141   \n2022-06-04 12:00:00         0.030        -0.081   -0.512     0.007    -0.027   \n2022-06-05 00:00:00        -0.118        -0.117    0.628     0.018    -0.096   \n\n                     error_12  error_13  error_14  day_8_error  \n2022-06-03 00:00:00    -0.221    -0.289     0.201       -0.094  \n2022-06-03 12:00:00     0.016    -0.341    -0.200        0.158  \n2022-06-04 00:00:00    -0.121     0.137     0.011       -0.031  \n2022-06-04 12:00:00    -0.326    -0.303    -0.023        0.147  \n2022-06-05 00:00:00     0.174     0.147    -0.022       -0.065  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>cmc-ens_14</th>\n      <th>ecmwf_diff_8</th>\n      <th>ecmwf_diff_9</th>\n      <th>error_9</th>\n      <th>error_10</th>\n      <th>error_11</th>\n      <th>error_12</th>\n      <th>error_13</th>\n      <th>error_14</th>\n      <th>day_8_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2022-06-03 00:00:00</th>\n      <td>-0.056</td>\n      <td>0.284</td>\n      <td>0.235</td>\n      <td>-0.012</td>\n      <td>-0.186</td>\n      <td>-0.254</td>\n      <td>-0.062</td>\n      <td>-0.088</td>\n      <td>-0.044</td>\n      <td>0.043</td>\n      <td>...</td>\n      <td>0.115</td>\n      <td>-0.089</td>\n      <td>-0.174</td>\n      <td>-0.140</td>\n      <td>-0.468</td>\n      <td>0.275</td>\n      <td>-0.221</td>\n      <td>-0.289</td>\n      <td>0.201</td>\n      <td>-0.094</td>\n    </tr>\n    <tr>\n      <th>2022-06-03 12:00:00</th>\n      <td>0.245</td>\n      <td>0.149</td>\n      <td>0.015</td>\n      <td>-0.177</td>\n      <td>-0.229</td>\n      <td>-0.307</td>\n      <td>0.023</td>\n      <td>-0.067</td>\n      <td>-0.048</td>\n      <td>0.265</td>\n      <td>...</td>\n      <td>0.162</td>\n      <td>-0.023</td>\n      <td>-0.057</td>\n      <td>0.297</td>\n      <td>-0.509</td>\n      <td>0.013</td>\n      <td>0.016</td>\n      <td>-0.341</td>\n      <td>-0.200</td>\n      <td>0.158</td>\n    </tr>\n    <tr>\n      <th>2022-06-04 00:00:00</th>\n      <td>0.091</td>\n      <td>0.101</td>\n      <td>-0.160</td>\n      <td>-0.319</td>\n      <td>-0.295</td>\n      <td>-0.112</td>\n      <td>-0.053</td>\n      <td>0.052</td>\n      <td>0.277</td>\n      <td>0.186</td>\n      <td>...</td>\n      <td>-0.024</td>\n      <td>-0.017</td>\n      <td>-0.042</td>\n      <td>-0.233</td>\n      <td>0.392</td>\n      <td>-0.141</td>\n      <td>-0.121</td>\n      <td>0.137</td>\n      <td>0.011</td>\n      <td>-0.031</td>\n    </tr>\n    <tr>\n      <th>2022-06-04 12:00:00</th>\n      <td>0.102</td>\n      <td>-0.112</td>\n      <td>-0.176</td>\n      <td>-0.194</td>\n      <td>-0.116</td>\n      <td>-0.056</td>\n      <td>-0.052</td>\n      <td>0.000</td>\n      <td>0.228</td>\n      <td>0.071</td>\n      <td>...</td>\n      <td>-0.171</td>\n      <td>0.030</td>\n      <td>-0.081</td>\n      <td>-0.512</td>\n      <td>0.007</td>\n      <td>-0.027</td>\n      <td>-0.326</td>\n      <td>-0.303</td>\n      <td>-0.023</td>\n      <td>0.147</td>\n    </tr>\n    <tr>\n      <th>2022-06-05 00:00:00</th>\n      <td>0.016</td>\n      <td>-0.119</td>\n      <td>-0.257</td>\n      <td>-0.154</td>\n      <td>-0.074</td>\n      <td>0.054</td>\n      <td>0.053</td>\n      <td>0.370</td>\n      <td>0.110</td>\n      <td>-0.109</td>\n      <td>...</td>\n      <td>0.240</td>\n      <td>-0.118</td>\n      <td>-0.117</td>\n      <td>0.628</td>\n      <td>0.018</td>\n      <td>-0.096</td>\n      <td>0.174</td>\n      <td>0.147</td>\n      <td>-0.022</td>\n      <td>-0.065</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_predictor = MultilabelPredictor.load(save_path)\n",
    "test_data_nolab = test_data.drop(columns=labels)\n",
    "test_data_nolab.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.426622Z",
     "end_time": "2023-06-06T13:38:01.470103Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with TabularPredictor for label: ecmwf-eps_9 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_10 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_11 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_12 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_13 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_14 ...\n",
      "Predictions:  \n",
      "                      ecmwf-eps_9  ecmwf-eps_10  ecmwf-eps_11  ecmwf-eps_12  \\\n",
      "2022-06-03 00:00:00    -0.070274     -0.054683     -0.014851     -0.040611   \n",
      "2022-06-03 12:00:00     0.019687      0.005112      0.005042     -0.007257   \n",
      "2022-06-04 00:00:00    -0.012484     -0.004831     -0.020349     -0.035969   \n",
      "2022-06-04 12:00:00     0.028655     -0.030362     -0.029364     -0.029605   \n",
      "2022-06-05 00:00:00    -0.070877     -0.065110     -0.076699     -0.045437   \n",
      "...                          ...           ...           ...           ...   \n",
      "2023-05-15 00:00:00     0.100431      0.041367     -0.120704     -0.264018   \n",
      "2023-05-15 12:00:00    -0.081939     -0.108349     -0.255811      0.007999   \n",
      "2023-05-16 00:00:00    -0.024271     -0.127283     -0.094130     -0.036875   \n",
      "2023-05-16 12:00:00     0.059725      0.194602      0.297363      0.002931   \n",
      "2023-05-17 00:00:00    -0.004628     -0.104407     -0.038733      0.074933   \n",
      "\n",
      "                     ecmwf-eps_13  ecmwf-eps_14  \n",
      "2022-06-03 00:00:00     -0.071666     -0.059891  \n",
      "2022-06-03 12:00:00     -0.004504      0.005896  \n",
      "2022-06-04 00:00:00     -0.052488     -0.039087  \n",
      "2022-06-04 12:00:00     -0.030316     -0.009782  \n",
      "2022-06-05 00:00:00     -0.031931      0.009311  \n",
      "...                           ...           ...  \n",
      "2023-05-15 00:00:00     -0.123066      0.190095  \n",
      "2023-05-15 12:00:00      0.010169     -0.016418  \n",
      "2023-05-16 00:00:00     -0.035306     -0.060014  \n",
      "2023-05-16 12:00:00     -0.175378     -0.194028  \n",
      "2023-05-17 00:00:00      0.110592     -0.084899  \n",
      "\n",
      "[697 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions = multi_predictor.predict(test_data_nolab)\n",
    "print(\"Predictions:  \\n\", predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:01.441719Z",
     "end_time": "2023-06-06T13:38:04.231681Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "ecmwf-eps_9    -0.004628\necmwf-eps_10   -0.104407\necmwf-eps_11   -0.038733\necmwf-eps_12    0.074933\necmwf-eps_13    0.110592\necmwf-eps_14   -0.084899\nName: 2023-05-17 00:00:00, dtype: float32"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.iloc[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:04.231152Z",
     "end_time": "2023-06-06T13:38:04.236811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "ecmwf-eps_9     0.000\necmwf-eps_10   -0.169\necmwf-eps_11   -0.271\necmwf-eps_12    0.025\necmwf-eps_13   -0.008\necmwf-eps_14   -0.078\nName: 2023-05-17 00:00:00, dtype: float64"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[labels].iloc[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:04.238526Z",
     "end_time": "2023-06-06T13:38:04.242195Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.08023007545311743\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.08023007545311743,\n",
      "    \"mean_squared_error\": -0.006436865007212917,\n",
      "    \"mean_absolute_error\": -0.05291710419293033,\n",
      "    \"r2\": 0.8108099725454907,\n",
      "    \"pearsonr\": 0.9010937094084588,\n",
      "    \"median_absolute_error\": -0.028835937261582956\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -0.11082954443475555\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11082954443475555,\n",
      "    \"mean_squared_error\": -0.012283187919615454,\n",
      "    \"mean_absolute_error\": -0.07472288912346754,\n",
      "    \"r2\": 0.6756541939793443,\n",
      "    \"pearsonr\": 0.8232688610737003,\n",
      "    \"median_absolute_error\": -0.04603454846143684\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.21695129123551476\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21695129123551476,\n",
      "    \"mean_squared_error\": -0.04706786276875714,\n",
      "    \"mean_absolute_error\": -0.14032097252547607,\n",
      "    \"r2\": 0.3058559574307328,\n",
      "    \"pearsonr\": 0.553666118289079,\n",
      "    \"median_absolute_error\": -0.07729820585250913\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_12 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.358465200581571\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.358465200581571,\n",
      "    \"mean_squared_error\": -0.12849730002798593,\n",
      "    \"mean_absolute_error\": -0.2250503711142715,\n",
      "    \"r2\": 0.14844945212599936,\n",
      "    \"pearsonr\": 0.3869007581003848,\n",
      "    \"median_absolute_error\": -0.13130800652503893\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_13 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.4945915091799656\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4945915091799656,\n",
      "    \"mean_squared_error\": -0.24462076095291596,\n",
      "    \"mean_absolute_error\": -0.31190416122347253,\n",
      "    \"r2\": 0.12786390743800213,\n",
      "    \"pearsonr\": 0.3582405521096596,\n",
      "    \"median_absolute_error\": -0.17864633360505106\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.6084669201391874\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6084669201391874,\n",
      "    \"mean_squared_error\": -0.37023199290366826,\n",
      "    \"mean_absolute_error\": -0.39079344333281113,\n",
      "    \"r2\": 0.12233431197612399,\n",
      "    \"pearsonr\": 0.350831916172944,\n",
      "    \"median_absolute_error\": -0.22501058137416763\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_14 ...\n",
      "Evaluated using metrics: {'ecmwf-eps_9': root_mean_squared_error, 'ecmwf-eps_10': root_mean_squared_error, 'ecmwf-eps_11': root_mean_squared_error, 'ecmwf-eps_12': root_mean_squared_error, 'ecmwf-eps_13': root_mean_squared_error, 'ecmwf-eps_14': root_mean_squared_error}\n"
     ]
    }
   ],
   "source": [
    "evaluations = multi_predictor.evaluate(test_data)\n",
    "#print(evaluations)\n",
    "print(\"Evaluated using metrics:\", multi_predictor.eval_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:04.242940Z",
     "end_time": "2023-06-06T13:38:07.842870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ecmwf-eps_9': {'root_mean_squared_error': -0.08023007545311743,\n  'mean_squared_error': -0.006436865007212917,\n  'mean_absolute_error': -0.05291710419293033,\n  'r2': 0.8108099725454907,\n  'pearsonr': 0.9010937094084588,\n  'median_absolute_error': -0.028835937261582956},\n 'ecmwf-eps_10': {'root_mean_squared_error': -0.11082954443475555,\n  'mean_squared_error': -0.012283187919615454,\n  'mean_absolute_error': -0.07472288912346754,\n  'r2': 0.6756541939793443,\n  'pearsonr': 0.8232688610737003,\n  'median_absolute_error': -0.04603454846143684},\n 'ecmwf-eps_11': {'root_mean_squared_error': -0.21695129123551476,\n  'mean_squared_error': -0.04706786276875714,\n  'mean_absolute_error': -0.14032097252547607,\n  'r2': 0.3058559574307328,\n  'pearsonr': 0.553666118289079,\n  'median_absolute_error': -0.07729820585250913},\n 'ecmwf-eps_12': {'root_mean_squared_error': -0.358465200581571,\n  'mean_squared_error': -0.12849730002798593,\n  'mean_absolute_error': -0.2250503711142715,\n  'r2': 0.14844945212599936,\n  'pearsonr': 0.3869007581003848,\n  'median_absolute_error': -0.13130800652503893},\n 'ecmwf-eps_13': {'root_mean_squared_error': -0.4945915091799656,\n  'mean_squared_error': -0.24462076095291596,\n  'mean_absolute_error': -0.31190416122347253,\n  'r2': 0.12786390743800213,\n  'pearsonr': 0.3582405521096596,\n  'median_absolute_error': -0.17864633360505106},\n 'ecmwf-eps_14': {'root_mean_squared_error': -0.6084669201391874,\n  'mean_squared_error': -0.37023199290366826,\n  'mean_absolute_error': -0.39079344333281113,\n  'r2': 0.12233431197612399,\n  'pearsonr': 0.350831916172944,\n  'median_absolute_error': -0.22501058137416763}}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:38:07.844050Z",
     "end_time": "2023-06-06T13:38:07.849106Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictor_class = multi_predictor.get_predictor('ecmwf-eps_13')\n",
    "predictor_class.leaderboard(silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T13:18:52.303311Z",
     "end_time": "2023-06-06T13:18:52.335126Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T12:30:54.593026Z",
     "end_time": "2023-06-06T12:30:54.598682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
