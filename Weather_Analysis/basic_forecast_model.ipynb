{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:26:56.632466Z",
     "end_time": "2023-06-07T12:26:58.045528Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import copy\n",
    "from meteostat import Stations, Daily\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics import R2Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = \"RawData\"\n",
    "\n",
    "\n",
    "def extract_date_time(filename):\n",
    "    \"\"\"\n",
    "    extract the date and time from the filename\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parts = filename.split('.')\n",
    "    date = parts[1]\n",
    "    time = parts[2]\n",
    "    return date, time\n",
    "\n",
    "\n",
    "def get_date(df, file):\n",
    "    \"\"\"get the date from the dataframe and the time from the filename and combine them into a datetime object\n",
    "    :param df: dataframe containing the date\n",
    "    :param file: filename containing the time\n",
    "    :return: datetime object\n",
    "    \"\"\"\n",
    "    #date_str = df[df.iloc[:, 2] == 1].iloc[0]['Date']\n",
    "    date_str = str(file.split('.')[1])\n",
    "    time_str = str(file.split('.')[2])\n",
    "    #date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    date = datetime.strptime(date_str, '%Y%m%d')\n",
    "    time_value = time(int(time_str), 0)\n",
    "    combined_datetime = datetime.combine(date.date(), time_value)\n",
    "    return combined_datetime\n",
    "\n",
    "\n",
    "degree_days = 'gw_hdd'\n",
    "ecmwf_files = glob.glob(path + f'/ecmwf.*.[01][02].{degree_days}.csv')\n",
    "ecmwf_sorted_files = sorted(ecmwf_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[3:]\n",
    "\n",
    "ecmwf_eps_files = glob.glob(path + f'/ecmwf-eps.*.[01][02].{degree_days}.csv')\n",
    "ecmwf_eps_sorted_files = sorted(ecmwf_eps_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "\n",
    "gfs_ens_bc_files = glob.glob(path + f'/gfs-ens-bc.*.[01][02].{degree_days}.csv')\n",
    "gfs_ens_bc_sorted_files = sorted(gfs_ens_bc_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "\n",
    "cmc_ens_files = glob.glob(path + f'/cmc-ens.*.[01][02].{degree_days}.csv')\n",
    "cmc_ens_sorted_files = sorted(cmc_ens_files, key=lambda x: (x.split('.')[1], x.split('.')[2]))[2:]\n",
    "for _ in range(2):\n",
    "    set1 = set((extract_date_time(filename) for filename in ecmwf_sorted_files))\n",
    "    set2 = set((extract_date_time(filename) for filename in ecmwf_eps_sorted_files))\n",
    "\n",
    "    ecmwf_sorted_files = [filename for filename in ecmwf_sorted_files if extract_date_time(filename) in set2]\n",
    "    ecmwf_eps_sorted_files = [filename for filename in ecmwf_eps_sorted_files if extract_date_time(filename) in set1]\n",
    "    cmc_ens_sorted_files = [filename for filename in cmc_ens_sorted_files if extract_date_time(filename) in set1]\n",
    "\n",
    "    master_set = set((extract_date_time(filename) for filename in cmc_ens_sorted_files))\n",
    "    gfs_ens_bc_sorted_files = [filename for filename in gfs_ens_bc_sorted_files if\n",
    "                               extract_date_time(filename) in master_set]\n",
    "\n",
    "    master_set = set((extract_date_time(filename) for filename in gfs_ens_bc_sorted_files))\n",
    "\n",
    "    ecmwf_sorted_files = [filename for filename in ecmwf_sorted_files if extract_date_time(filename) in master_set]\n",
    "    ecmwf_eps_sorted_files = [filename for filename in ecmwf_eps_sorted_files if\n",
    "                              extract_date_time(filename) in master_set]\n",
    "    gfs_ens_bc_sorted_files = [filename for filename in gfs_ens_bc_sorted_files if\n",
    "                               extract_date_time(filename) in master_set]\n",
    "    cmc_ens_sorted_files = [filename for filename in cmc_ens_sorted_files if extract_date_time(filename) in master_set]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T08:59:52.132079Z",
     "end_time": "2023-06-07T08:59:53.294932Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ecmwf_eps_change_df = pd.DataFrame(columns=['ecmwf-eps_9', 'ecmwf-eps_10', 'ecmwf-eps_11', 'ecmwf-eps_12',\n",
    "                                  'ecmwf-eps_13', 'ecmwf-eps_14'])\n",
    "passed_rows = []\n",
    "for i in range(1, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            changes.append(ecmwf_eps_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=ecmwf_eps_change_df.columns, index=[date])\n",
    "        ecmwf_eps_change_df = pd.concat([ecmwf_eps_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:38:08.916600Z",
     "end_time": "2023-06-07T10:38:18.553953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:38:18.555149Z",
     "end_time": "2023-06-07T10:38:18.558190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:38:18.558814Z",
     "end_time": "2023-06-07T10:38:18.560070Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ecmwf_change_df = pd.DataFrame(columns=['ecmwf_diff_8', 'ecmwf_diff_9',])\n",
    "passed_rows = []\n",
    "for i in range(1, len(ecmwf_sorted_files)):\n",
    "    ecmwf_df = pd.read_csv(ecmwf_sorted_files[i])\n",
    "    ecmwf_df = ecmwf_df[ecmwf_df[ecmwf_df.columns[2]] >= 1]\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    try:\n",
    "        ecmwf = ecmwf_df.iloc[8]\n",
    "        ecmwf_eps = ecmwf_eps_df.iloc[9]\n",
    "    except IndexError:\n",
    "        print(f\"error on row: {i}\")\n",
    "        passed_rows.append(i)\n",
    "        continue\n",
    "\n",
    "    date = get_date(ecmwf_df, ecmwf_sorted_files[i])\n",
    "    prev_date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8,10):\n",
    "            changes.append(ecmwf_df.iloc[day - offset]['Value'] - ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=ecmwf_change_df.columns, index=[date])\n",
    "        ecmwf_change_df = pd.concat([ecmwf_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:38:18.561211Z",
     "end_time": "2023-06-07T10:38:25.986326Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:38:25.987096Z",
     "end_time": "2023-06-07T10:38:25.990307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:36:35.241780Z",
     "end_time": "2023-06-07T10:36:35.250108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:38:40.202046Z",
     "end_time": "2023-06-07T10:38:40.205159Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gfs_ens_bc_change_df = pd.DataFrame(columns=['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12',\n",
    "                                  'gfs-ens-bc_13', 'gfs-ens-bc_14'])\n",
    "passed_rows = []\n",
    "for i in range(1, len(gfs_ens_bc_sorted_files)):\n",
    "    gfs_ens_bc_df = pd.read_csv(gfs_ens_bc_sorted_files[i])\n",
    "    gfs_ens_bc_df = gfs_ens_bc_df[gfs_ens_bc_df[gfs_ens_bc_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    try:\n",
    "        date = get_date(gfs_ens_bc_df, gfs_ens_bc_sorted_files[i])\n",
    "        prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    except IndexError:\n",
    "        print(f\"error on row: {i}\")\n",
    "        passed_rows.append(i)\n",
    "        continue\n",
    "\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            changes.append(gfs_ens_bc_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=gfs_ens_bc_change_df.columns, index=[date])\n",
    "        gfs_ens_bc_change_df = pd.concat([gfs_ens_bc_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:40:47.022122Z",
     "end_time": "2023-06-07T10:40:57.034356Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:41:10.608616Z",
     "end_time": "2023-06-07T10:41:10.611842Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmc_ens_change_df = pd.DataFrame(columns=['cmc-ens_9', 'cmc-ens_10', 'cmc-ens_11', 'cmc-ens_12',\n",
    "                                  'cmc-ens_13', 'cmc-ens_14'])\n",
    "passed_rows = []\n",
    "\n",
    "for i in range(1, len(cmc_ens_sorted_files)):\n",
    "    cmc_ens_df = pd.read_csv(cmc_ens_sorted_files[i])\n",
    "    cmc_ens_df = cmc_ens_df[cmc_ens_df[cmc_ens_df.columns[2]] >= 1]\n",
    "    gfs_ens_bc_df = pd.read_csv(gfs_ens_bc_sorted_files[i])\n",
    "    gfs_ens_bc_df = gfs_ens_bc_df[gfs_ens_bc_df[gfs_ens_bc_df.columns[2]] >= 1]\n",
    "    date = get_date(cmc_ens_df, cmc_ens_sorted_files[i])\n",
    "\n",
    "    changes = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            changes.append(cmc_ens_df.iloc[day]['Value'] - gfs_ens_bc_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=cmc_ens_change_df.columns, index=[date])\n",
    "        cmc_ens_change_df = pd.concat([cmc_ens_change_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:44:59.501122Z",
     "end_time": "2023-06-07T10:45:09.128016Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "passed_rows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:45:14.442944Z",
     "end_time": "2023-06-07T10:45:14.446312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "day_8_error = pd.DataFrame(columns=['day_8_error'])\n",
    "passed_rows = []\n",
    "\n",
    "for i in range(1, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "    day = 7\n",
    "    changes = []\n",
    "    try:\n",
    "        changes.append(ecmwf_eps_df.iloc[day]['Value'] - prev_ecmwf_eps_df.iloc[day + offset]['Value'])\n",
    "        new_row = pd.DataFrame([changes], columns=day_8_error.columns, index=[date])\n",
    "        day_8_error = pd.concat([day_8_error, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:48:09.397638Z",
     "end_time": "2023-06-07T10:48:16.685824Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "errors_df = pd.DataFrame(columns=['error_9', 'error_10', 'error_11', 'error_12', 'error_13', 'error_14'])\n",
    "passed_rows = []\n",
    "\n",
    "for i in range(2, len(ecmwf_eps_sorted_files)):\n",
    "    ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-1])\n",
    "    ecmwf_eps_df = ecmwf_eps_df[ecmwf_eps_df[ecmwf_eps_df.columns[2]] >= 1]\n",
    "    prev_ecmwf_eps_df = pd.read_csv(ecmwf_eps_sorted_files[i-2])\n",
    "    prev_ecmwf_eps_df = prev_ecmwf_eps_df[prev_ecmwf_eps_df[prev_ecmwf_eps_df.columns[2]] >= 1]\n",
    "\n",
    "    date = get_date(ecmwf_eps_df, ecmwf_eps_sorted_files[i])\n",
    "    prev_date = get_date(prev_ecmwf_eps_df, ecmwf_eps_sorted_files[i-1])\n",
    "    d2 = str(date)[:10]\n",
    "    d1 = str(prev_date)[:10]\n",
    "\n",
    "    if d2 != d1:\n",
    "        offset = 1\n",
    "    else:\n",
    "        offset = 0\n",
    "\n",
    "    errors = []\n",
    "    try:\n",
    "        for day in range(8, 14):\n",
    "            errors.append(ecmwf_eps_df.iloc[day - offset]['Value'] - prev_ecmwf_eps_df.iloc[day]['Value'])\n",
    "        new_row = pd.DataFrame([errors], columns=errors_df.columns, index=[date])\n",
    "        errors_df = pd.concat([errors_df, new_row])\n",
    "    except IndexError:\n",
    "        print(f\"error on {date}\")\n",
    "        passed_rows.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:50:09.315926Z",
     "end_time": "2023-06-07T10:50:18.368675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:50:18.369132Z",
     "end_time": "2023-06-07T10:50:18.371146Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "master_df = pd.concat([gfs_ens_bc_change_df, cmc_ens_change_df, ecmwf_change_df, errors_df, day_8_error, ecmwf_eps_change_df], axis=1)\n",
    "master_df.fillna(0, inplace=True)\n",
    "display(master_df[-45:-35])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:50:59.394519Z",
     "end_time": "2023-06-07T10:50:59.416720Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "master_df.to_pickle('master_df.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:11.363027Z",
     "end_time": "2023-06-07T10:51:11.377615Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "master_df = pd.read_pickle('master_df.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:06.881484Z",
     "end_time": "2023-06-07T12:27:06.888623Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:13.732847Z",
     "end_time": "2023-06-07T10:51:13.735675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = master_df.iloc[:, :-6]\n",
    "y = master_df.iloc[:, -6:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:14.008246Z",
     "end_time": "2023-06-07T10:51:14.013047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:14.671726Z",
     "end_time": "2023-06-07T10:51:14.674477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, max_depth=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:51:14.851932Z",
     "end_time": "2023-06-07T10:52:01.639354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:52:01.641000Z",
     "end_time": "2023-06-07T10:52:02.138638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_features = master_df.iloc[:, :-6].values ** 2\n",
    "target_variables = master_df.iloc[:, -6:].values\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_features, target_variables, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale the input features based on the training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test data based on the training data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data to PyTorch tensors\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train_scaled)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "X_val_tensor = torch.Tensor(X_val_scaled)\n",
    "y_val_tensor = torch.Tensor(y_val)\n",
    "X_test_tensor = torch.Tensor(X_test_scaled)\n",
    "y_test_tensor = torch.Tensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:57:17.393078Z",
     "end_time": "2023-06-07T11:57:17.402574Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#benchmark error\n",
    "total_mse = 0\n",
    "c = 0\n",
    "for i in range(1, len(y_test_tensor)):\n",
    "    #mse = mean_squared_error(y_test_tensor[i], y_test_tensor[i-1])\n",
    "    mse = mean_squared_error(y_test_tensor[i], [0,0,0,0,0,0])\n",
    "    total_mse += mse\n",
    "    c += 1\n",
    "\n",
    "total_mse/c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:57:17.571212Z",
     "end_time": "2023-06-07T11:57:17.642334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, output_size)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = F.relu(out)  # Apply ReLU activation between LSTM and first dense layer\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = F.relu(out)  # Apply ReLU activation to the output of the first dense layer\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:57:18.215284Z",
     "end_time": "2023-06-07T11:57:18.221355Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = y_train_tensor.shape[1]\n",
    "hidden_size = 256\n",
    "num_layers = 3\n",
    "dropout = 0.3\n",
    "lr = 0.01\n",
    "mps_device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:58:01.427352Z",
     "end_time": "2023-06-07T11:58:01.432498Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:04:37.978958Z",
     "end_time": "2023-06-07T11:04:37.991726Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "sequence_length = 10  # Number of previous days to consider\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loop through each sequence in the training data\n",
    "    for i in range(sequence_length, X_train_tensor.shape[0]):\n",
    "        # Extract the current sequence and target\n",
    "        input_seq = X_train_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "        target_seq = y_train_tensor[i]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / (X_train_tensor.shape[0] - sequence_length)\n",
    "    train_losses.append(average_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {average_loss}')\n",
    "\n",
    "    # Validation stage\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for i in range(sequence_length, X_val_tensor.shape[0]):\n",
    "            input_seq = X_val_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "            target_seq = y_val_tensor[i]\n",
    "\n",
    "            output = model(input_seq)\n",
    "            val_loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        average_val_loss = total_val_loss / (X_val_tensor.shape[0] - sequence_length)\n",
    "        val_losses.append(average_val_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {average_val_loss}')\n",
    "\n",
    "        # Check if current model is the best based on validation loss\n",
    "        if average_val_loss < best_loss:\n",
    "            best_loss = average_val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "# After training, use the best model for testing\n",
    "model = best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T10:52:26.272553Z",
     "end_time": "2023-06-07T10:59:06.610026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(train_losses, label='Training Loss')\n",
    "    ax.plot(val_losses, label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "plot_losses(train_losses, val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:40.798333Z",
     "end_time": "2023-06-07T11:29:40.955675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(sequence_length, X_test_tensor.shape[0]):\n",
    "        input_seq = X_test_tensor[i - sequence_length:i].view(1, sequence_length, -1)\n",
    "        target_seq = y_test_tensor[i]\n",
    "\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_seq.unsqueeze(0))\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Extract the scalar value from the tensor and append it to predictions\n",
    "        predictions.append(output.squeeze().tolist())\n",
    "\n",
    "    average_test_loss = test_loss / (X_test_tensor.shape[0] - sequence_length)\n",
    "    print(f'Test Loss: {average_test_loss}')\n",
    "\n",
    "    # Convert the predictions and target values to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    targets = y_test_tensor[sequence_length:].numpy()\n",
    "\n",
    "    # Evaluate the performance using appropriate metrics\n",
    "    # For example, calculate mean squared error (MSE)\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    metric = R2Score()\n",
    "    r2 = metric.update(torch.tensor(predictions), torch.tensor(targets)).compute()\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'R2 Score: {r2}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:42.167101Z",
     "end_time": "2023-06-07T11:29:44.081810Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:50.774388Z",
     "end_time": "2023-06-07T11:29:50.781270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prediction(input):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor(input).view(1, sequence_length, -1)\n",
    "        output = model(input_seq)\n",
    "        return output.squeeze().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:29:50.970175Z",
     "end_time": "2023-06-07T11:29:50.974005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = prediction(X_test_tensor[-sequence_length:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:30:05.822139Z",
     "end_time": "2023-06-07T11:30:05.837688Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:30:06.606259Z",
     "end_time": "2023-06-07T11:30:06.609147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test_tensor[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T11:30:07.257100Z",
     "end_time": "2023-06-07T11:30:07.266004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:13:04.741373Z",
     "end_time": "2023-06-07T12:13:04.749586Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autogloun"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "import os.path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:15.438556Z",
     "end_time": "2023-06-07T12:27:15.608441Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:15.905086Z",
     "end_time": "2023-06-07T12:27:15.910639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "y = master_df.iloc[:, -6:].copy()\n",
    "y = y.reset_index(drop=True)\n",
    "X = master_df.iloc[:, :-6].copy()\n",
    "X['Date'] = X.index\n",
    "X = X.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:16.254244Z",
     "end_time": "2023-06-07T12:27:16.267925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "X = auto_ml_pipeline_feature_generator.fit_transform(X=X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:43.881033Z",
     "end_time": "2023-06-07T12:27:43.913498Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "      gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  gfs-ens-bc_12  \\\n0            0.012          0.009          0.006          0.002   \n1            0.001          0.001          0.004          0.009   \n2            0.010          0.007          0.011          0.013   \n3            0.006          0.009          0.010          0.009   \n4            0.012          0.007          0.008          0.007   \n...            ...            ...            ...            ...   \n3477         1.370          0.961          0.180          0.107   \n3478        -1.058         -0.058          0.440          0.334   \n3479        -0.018         -0.324         -0.084          0.108   \n3480        -0.719         -0.732         -0.481         -0.186   \n3481        -0.135         -0.121         -0.083         -0.034   \n\n      gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  cmc-ens_10  cmc-ens_11  \\\n0             0.004          0.011     -0.008      -0.005      -0.001   \n1             0.011          0.013     -0.003      -0.002      -0.004   \n2             0.011          0.008     -0.007      -0.011      -0.013   \n3             0.009          0.009     -0.008      -0.009      -0.009   \n4             0.009          0.009     -0.007      -0.009      -0.009   \n...             ...            ...        ...         ...         ...   \n3477          0.123          0.015     -0.752      -0.071      -0.003   \n3478          0.098         -0.206     -0.139      -0.626      -0.916   \n3479          0.380          0.401      0.330      -0.207      -0.039   \n3480          0.040          0.155     -0.360      -0.185      -0.309   \n3481         -0.196         -0.119      0.325       0.155       0.067   \n\n      cmc-ens_12  ...  error_11  error_12  error_13  error_14  day_8_error  \\\n0         -0.003  ...     0.000     0.000     0.000     0.000        0.000   \n1         -0.008  ...     0.000     0.000     0.001    -0.001        0.005   \n2         -0.011  ...     0.001     0.000    -0.001     0.000        0.000   \n3         -0.010  ...     0.000     0.001     0.001    -0.001       -0.002   \n4         -0.010  ...     0.000     0.000     0.001     0.001        0.000   \n...          ...  ...       ...       ...       ...       ...          ...   \n3477       0.156  ...     0.076     0.004    -0.137    -0.039        0.985   \n3478      -0.714  ...    -0.115    -0.153    -0.300    -0.210       -0.607   \n3479       0.085  ...     0.590     0.388     0.165    -0.251        0.375   \n3480      -0.252  ...    -0.003    -0.011     0.082     0.179       -0.624   \n3481      -0.282  ...    -0.365    -0.078     0.174     0.379        0.482   \n\n                     Date  Date.year  Date.month  Date.day  Date.dayofweek  \n0     1531267200000000000       2018           7        11               2  \n1     1531310400000000000       2018           7        11               2  \n2     1531353600000000000       2018           7        12               3  \n3     1531396800000000000       2018           7        12               3  \n4     1531440000000000000       2018           7        13               4  \n...                   ...        ...         ...       ...             ...  \n3477  1684108800000000000       2023           5        15               0  \n3478  1684152000000000000       2023           5        15               0  \n3479  1684195200000000000       2023           5        16               1  \n3480  1684238400000000000       2023           5        16               1  \n3481  1684281600000000000       2023           5        17               2  \n\n[3482 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>error_11</th>\n      <th>error_12</th>\n      <th>error_13</th>\n      <th>error_14</th>\n      <th>day_8_error</th>\n      <th>Date</th>\n      <th>Date.year</th>\n      <th>Date.month</th>\n      <th>Date.day</th>\n      <th>Date.dayofweek</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.012</td>\n      <td>0.009</td>\n      <td>0.006</td>\n      <td>0.002</td>\n      <td>0.004</td>\n      <td>0.011</td>\n      <td>-0.008</td>\n      <td>-0.005</td>\n      <td>-0.001</td>\n      <td>-0.003</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>1531267200000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>11</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.004</td>\n      <td>0.009</td>\n      <td>0.011</td>\n      <td>0.013</td>\n      <td>-0.003</td>\n      <td>-0.002</td>\n      <td>-0.004</td>\n      <td>-0.008</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>0.005</td>\n      <td>1531310400000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>11</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.010</td>\n      <td>0.007</td>\n      <td>0.011</td>\n      <td>0.013</td>\n      <td>0.011</td>\n      <td>0.008</td>\n      <td>-0.007</td>\n      <td>-0.011</td>\n      <td>-0.013</td>\n      <td>-0.011</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>1531353600000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.006</td>\n      <td>0.009</td>\n      <td>0.010</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.008</td>\n      <td>-0.009</td>\n      <td>-0.009</td>\n      <td>-0.010</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>-0.001</td>\n      <td>-0.002</td>\n      <td>1531396800000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.012</td>\n      <td>0.007</td>\n      <td>0.008</td>\n      <td>0.007</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.007</td>\n      <td>-0.009</td>\n      <td>-0.009</td>\n      <td>-0.010</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>1531440000000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>13</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3477</th>\n      <td>1.370</td>\n      <td>0.961</td>\n      <td>0.180</td>\n      <td>0.107</td>\n      <td>0.123</td>\n      <td>0.015</td>\n      <td>-0.752</td>\n      <td>-0.071</td>\n      <td>-0.003</td>\n      <td>0.156</td>\n      <td>...</td>\n      <td>0.076</td>\n      <td>0.004</td>\n      <td>-0.137</td>\n      <td>-0.039</td>\n      <td>0.985</td>\n      <td>1684108800000000000</td>\n      <td>2023</td>\n      <td>5</td>\n      <td>15</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3478</th>\n      <td>-1.058</td>\n      <td>-0.058</td>\n      <td>0.440</td>\n      <td>0.334</td>\n      <td>0.098</td>\n      <td>-0.206</td>\n      <td>-0.139</td>\n      <td>-0.626</td>\n      <td>-0.916</td>\n      <td>-0.714</td>\n      <td>...</td>\n      <td>-0.115</td>\n      <td>-0.153</td>\n      <td>-0.300</td>\n      <td>-0.210</td>\n      <td>-0.607</td>\n      <td>1684152000000000000</td>\n      <td>2023</td>\n      <td>5</td>\n      <td>15</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3479</th>\n      <td>-0.018</td>\n      <td>-0.324</td>\n      <td>-0.084</td>\n      <td>0.108</td>\n      <td>0.380</td>\n      <td>0.401</td>\n      <td>0.330</td>\n      <td>-0.207</td>\n      <td>-0.039</td>\n      <td>0.085</td>\n      <td>...</td>\n      <td>0.590</td>\n      <td>0.388</td>\n      <td>0.165</td>\n      <td>-0.251</td>\n      <td>0.375</td>\n      <td>1684195200000000000</td>\n      <td>2023</td>\n      <td>5</td>\n      <td>16</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3480</th>\n      <td>-0.719</td>\n      <td>-0.732</td>\n      <td>-0.481</td>\n      <td>-0.186</td>\n      <td>0.040</td>\n      <td>0.155</td>\n      <td>-0.360</td>\n      <td>-0.185</td>\n      <td>-0.309</td>\n      <td>-0.252</td>\n      <td>...</td>\n      <td>-0.003</td>\n      <td>-0.011</td>\n      <td>0.082</td>\n      <td>0.179</td>\n      <td>-0.624</td>\n      <td>1684238400000000000</td>\n      <td>2023</td>\n      <td>5</td>\n      <td>16</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3481</th>\n      <td>-0.135</td>\n      <td>-0.121</td>\n      <td>-0.083</td>\n      <td>-0.034</td>\n      <td>-0.196</td>\n      <td>-0.119</td>\n      <td>0.325</td>\n      <td>0.155</td>\n      <td>0.067</td>\n      <td>-0.282</td>\n      <td>...</td>\n      <td>-0.365</td>\n      <td>-0.078</td>\n      <td>0.174</td>\n      <td>0.379</td>\n      <td>0.482</td>\n      <td>1684281600000000000</td>\n      <td>2023</td>\n      <td>5</td>\n      <td>17</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>3482 rows Ã— 26 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:27:48.292378Z",
     "end_time": "2023-06-07T12:27:48.327290Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = pd.concat([X, y], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:00.994903Z",
     "end_time": "2023-06-07T12:28:01.001275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:03.896234Z",
     "end_time": "2023-06-07T12:28:03.900718Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_len = 0.8\n",
    "train_data = TabularDataset(df[:int(len(df)*train_len)])\n",
    "test_data = TabularDataset(df[int(len(df)*train_len):])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:04.527067Z",
     "end_time": "2023-06-07T12:28:04.530337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "labels = ['ecmwf-eps_9', 'ecmwf-eps_10', 'ecmwf-eps_11', 'ecmwf-eps_12', 'ecmwf-eps_13',\n",
    "          'ecmwf-eps_14']\n",
    "save_path = 'models'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:06.741279Z",
     "end_time": "2023-06-07T12:28:06.750198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  gfs-ens-bc_12  gfs-ens-bc_13  \\\n0         0.012          0.009          0.006          0.002          0.004   \n1         0.001          0.001          0.004          0.009          0.011   \n2         0.010          0.007          0.011          0.013          0.011   \n3         0.006          0.009          0.010          0.009          0.009   \n4         0.012          0.007          0.008          0.007          0.009   \n\n   gfs-ens-bc_14  cmc-ens_9  cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  \\\n0          0.011     -0.008      -0.005      -0.001      -0.003  ...   \n1          0.013     -0.003      -0.002      -0.004      -0.008  ...   \n2          0.008     -0.007      -0.011      -0.013      -0.011  ...   \n3          0.009     -0.008      -0.009      -0.009      -0.010  ...   \n4          0.009     -0.007      -0.009      -0.009      -0.010  ...   \n\n   Date.year  Date.month  Date.day  Date.dayofweek  ecmwf-eps_9  ecmwf-eps_10  \\\n0       2018           7        11               2        0.000         0.002   \n1       2018           7        11               2        0.001         0.000   \n2       2018           7        12               3        0.000         0.003   \n3       2018           7        12               3       -0.003        -0.001   \n4       2018           7        13               4        0.000         0.000   \n\n   ecmwf-eps_11  ecmwf-eps_12  ecmwf-eps_13  ecmwf-eps_14  \n0         0.001         0.000         0.000         0.000  \n1         0.000         0.000        -0.001         0.001  \n2         0.001         0.000         0.001         0.000  \n3         0.001         0.001         0.000         0.000  \n4        -0.002        -0.002         0.001         0.004  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>Date.year</th>\n      <th>Date.month</th>\n      <th>Date.day</th>\n      <th>Date.dayofweek</th>\n      <th>ecmwf-eps_9</th>\n      <th>ecmwf-eps_10</th>\n      <th>ecmwf-eps_11</th>\n      <th>ecmwf-eps_12</th>\n      <th>ecmwf-eps_13</th>\n      <th>ecmwf-eps_14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.012</td>\n      <td>0.009</td>\n      <td>0.006</td>\n      <td>0.002</td>\n      <td>0.004</td>\n      <td>0.011</td>\n      <td>-0.008</td>\n      <td>-0.005</td>\n      <td>-0.001</td>\n      <td>-0.003</td>\n      <td>...</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>11</td>\n      <td>2</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.004</td>\n      <td>0.009</td>\n      <td>0.011</td>\n      <td>0.013</td>\n      <td>-0.003</td>\n      <td>-0.002</td>\n      <td>-0.004</td>\n      <td>-0.008</td>\n      <td>...</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>11</td>\n      <td>2</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.010</td>\n      <td>0.007</td>\n      <td>0.011</td>\n      <td>0.013</td>\n      <td>0.011</td>\n      <td>0.008</td>\n      <td>-0.007</td>\n      <td>-0.011</td>\n      <td>-0.013</td>\n      <td>-0.011</td>\n      <td>...</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>12</td>\n      <td>3</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.006</td>\n      <td>0.009</td>\n      <td>0.010</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.008</td>\n      <td>-0.009</td>\n      <td>-0.009</td>\n      <td>-0.010</td>\n      <td>...</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>12</td>\n      <td>3</td>\n      <td>-0.003</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.012</td>\n      <td>0.007</td>\n      <td>0.008</td>\n      <td>0.007</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.007</td>\n      <td>-0.009</td>\n      <td>-0.009</td>\n      <td>-0.010</td>\n      <td>...</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>13</td>\n      <td>4</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.002</td>\n      <td>-0.002</td>\n      <td>0.001</td>\n      <td>0.004</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:07.040973Z",
     "end_time": "2023-06-07T12:28:07.047695Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class MultilabelPredictor():\n",
    "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
    "        Creates multiple TabularPredictor objects which you can also use individually.\n",
    "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels : List[str]\n",
    "            The ith element of this list is the column (i.e. `label`) predicted by the ith TabularPredictor stored in this object.\n",
    "        path : str, default = None\n",
    "            Path to directory where models and intermediate outputs should be saved.\n",
    "            If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
    "            Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
    "            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
    "            Caution: when predicting many labels, this directory may grow large as it needs to store many TabularPredictors.\n",
    "        problem_types : List[str], default = None\n",
    "            The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
    "        eval_metrics : List[str], default = None\n",
    "            The ith element is the `eval_metric` for the ith TabularPredictor stored in this object.\n",
    "        consider_labels_correlation : bool, default = True\n",
    "            Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others.\n",
    "            If True, the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous labels appearing earlier in this list (i.e. in an auto-regressive fashion).\n",
    "            Set to False if during inference you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
    "        kwargs :\n",
    "            Arguments passed into the initialization of each TabularPredictor.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
    "\n",
    "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
    "        if len(labels) < 2:\n",
    "            raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
    "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
    "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
    "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
    "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
    "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
    "        self.labels = labels\n",
    "        self.consider_labels_correlation = consider_labels_correlation\n",
    "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
    "        if eval_metrics is None:\n",
    "            self.eval_metrics = {}\n",
    "        else:\n",
    "            self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
    "        problem_type = None\n",
    "        eval_metric = None\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            path_i = self.path + \"Predictor_\" + label\n",
    "            if problem_types is not None:\n",
    "                problem_type = problem_types[i]\n",
    "            if eval_metrics is not None:\n",
    "                eval_metric = eval_metrics[i]\n",
    "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
    "\n",
    "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
    "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                See documentation for `TabularPredictor.fit()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, str):\n",
    "            train_data = TabularDataset(train_data)\n",
    "        if tuning_data is not None and isinstance(tuning_data, str):\n",
    "            tuning_data = TabularDataset(tuning_data)\n",
    "        train_data_og = train_data.copy()\n",
    "        if tuning_data is not None:\n",
    "            tuning_data_og = tuning_data.copy()\n",
    "        else:\n",
    "            tuning_data_og = None\n",
    "        save_metrics = len(self.eval_metrics) == 0\n",
    "        for i in range(len(self.labels)):\n",
    "            label = self.labels[i]\n",
    "            predictor = self.get_predictor(label)\n",
    "            if not self.consider_labels_correlation:\n",
    "                labels_to_drop = [l for l in self.labels if l != label]\n",
    "            else:\n",
    "                labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
    "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
    "            if tuning_data is not None:\n",
    "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
    "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
    "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
    "            self.predictors[label] = predictor.path\n",
    "            if save_metrics:\n",
    "                self.eval_metrics[label] = predictor.eval_metric\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, data, **kwargs):\n",
    "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the predict() call for each TabularPredictor.\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=False, **kwargs)\n",
    "\n",
    "    def predict_proba(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
    "        \"\"\"\n",
    "        return self._predict(data, as_proba=True, **kwargs)\n",
    "\n",
    "    def evaluate(self, data, **kwargs):\n",
    "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
    "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
    "            kwargs :\n",
    "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
    "        \"\"\"\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
    "            if self.consider_labels_correlation:\n",
    "                data[label] = predictor.predict(data, **kwargs)\n",
    "        return eval_dict\n",
    "\n",
    "    def feature_imp(self,data, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        eval_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Evaluating feature importance for label: {label} ...\")\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
    "        for label in self.labels:\n",
    "            if not isinstance(self.predictors[label], str):\n",
    "                self.predictors[label] = self.predictors[label].path\n",
    "        save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
    "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
    "        path = os.path.expanduser(path)\n",
    "        if path[-1] != os.path.sep:\n",
    "            path = path + os.path.sep\n",
    "        return load_pkl.load(path=path+cls.multi_predictor_file)\n",
    "\n",
    "    def get_predictor(self, label):\n",
    "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
    "        predictor = self.predictors[label]\n",
    "        if isinstance(predictor, str):\n",
    "            return TabularPredictor.load(path=predictor)\n",
    "        return predictor\n",
    "\n",
    "    def _get_data(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return TabularDataset(data)\n",
    "        return data.copy()\n",
    "\n",
    "    def _predict(self, data, as_proba=False, **kwargs):\n",
    "        data = self._get_data(data)\n",
    "        if as_proba:\n",
    "            predproba_dict = {}\n",
    "        for label in self.labels:\n",
    "            print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
    "            predictor = self.get_predictor(label)\n",
    "            if as_proba:\n",
    "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
    "            data[label] = predictor.predict(data, **kwargs)\n",
    "        if not as_proba:\n",
    "            return data[self.labels]\n",
    "        else:\n",
    "            return predproba_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:07.285167Z",
     "end_time": "2023-06-07T12:28:07.303873Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_9\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_10\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_11\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_12\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_13\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"models/Predictor_ecmwf-eps_14\"\n"
     ]
    }
   ],
   "source": [
    "multi_predictor = MultilabelPredictor(labels=labels, path=save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:07.600361Z",
     "end_time": "2023-06-07T12:28:07.608605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"models/Predictor_ecmwf-eps_9/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 26\n",
      "Label Column: ecmwf-eps_9\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.132000000000005, -7.687000000000005, 0.01289, 1.61633)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1450.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 21 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 21 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t26 features in original data used to generate 26 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.8665\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-2.0637\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: ecmwf-eps_9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7002\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.6899\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.6844\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.7034\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.6932\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6929\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6522\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 14.6s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models/Predictor_ecmwf-eps_9/\")\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"models/Predictor_ecmwf-eps_10/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 27\n",
      "Label Column: ecmwf-eps_10\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (6.731999999999999, -7.844000000000001, 0.01838, 1.49726)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1392.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.6 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 22 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 22 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t27 features in original data used to generate 27 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.6 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.6517\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.8194\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: ecmwf-eps_10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9032\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.23s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7988\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.907\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.7624\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.8348\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.7489\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.91s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.7262\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 18.29s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models/Predictor_ecmwf-eps_10/\")\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"models/Predictor_ecmwf-eps_11/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 28\n",
      "Label Column: ecmwf-eps_11\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.802, -7.863000000000003, 0.02167, 1.37127)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1374.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.62 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 23 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 23 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t28 features in original data used to generate 28 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.62 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.5792\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.7234\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: ecmwf-eps_11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8561\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7826\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.8667\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.6875\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.8502\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6789\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6586\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models/Predictor_ecmwf-eps_11/\")\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"models/Predictor_ecmwf-eps_12/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 29\n",
      "Label Column: ecmwf-eps_12\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5.904999999999994, -6.439, 0.02074, 1.25962)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1389.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.65 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 24 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 24 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t29 features in original data used to generate 29 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.4088\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.5285\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: ecmwf-eps_12 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7919\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.39s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7367\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.7791\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.6341\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.7478\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.681\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6316\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 13.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models/Predictor_ecmwf-eps_12/\")\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"models/Predictor_ecmwf-eps_13/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 30\n",
      "Label Column: ecmwf-eps_13\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5.749000000000002, -5.777999999999995, 0.01271, 1.17099)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1398.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.67 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t30 features in original data used to generate 30 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.67 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.2626\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.3799\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: ecmwf-eps_13 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7193\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.6296\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.7076\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.5927\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.6874\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.5982\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.5733\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models/Predictor_ecmwf-eps_13/\")\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"models/Predictor_ecmwf-eps_14/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 31\n",
      "Label Column: ecmwf-eps_14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (4.6739999999999995, -5.354000000000003, 0.00665, 1.08207)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1389.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.69 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 26 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 26 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t31 features in original data used to generate 31 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.69 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.2221\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.3195\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TabularPredictor for label: ecmwf-eps_14 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6622\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.6072\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.6749\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.5591\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.6522\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6063\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.5567\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models/Predictor_ecmwf-eps_14/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('models/')\n"
     ]
    }
   ],
   "source": [
    "multi_predictor.fit(train_data) # add presets='best_quality' for better results, but longer runtime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:28:10.730068Z",
     "end_time": "2023-06-07T12:29:42.523679Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "      gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  gfs-ens-bc_12  \\\n2785        -0.324         -0.276         -0.110         -0.182   \n2786        -0.233         -0.238         -0.207         -0.074   \n2787        -0.164         -0.103         -0.039          0.044   \n2788        -0.224         -0.029         -0.032         -0.087   \n2789         0.238          0.247          0.328          0.372   \n\n      gfs-ens-bc_13  gfs-ens-bc_14  cmc-ens_9  cmc-ens_10  cmc-ens_11  \\\n2785         -0.039          0.143      0.272       0.178       0.121   \n2786          0.055          0.106     -0.070      -0.170      -0.144   \n2787          0.033          0.025      0.201      -0.113      -0.270   \n2788         -0.065          0.020     -0.119      -0.236      -0.202   \n2789          0.151         -0.013     -0.160      -0.389      -0.462   \n\n      cmc-ens_12  ...  error_11  error_12  error_13  error_14  day_8_error  \\\n2785       0.060  ...     0.088     0.016    -0.049    -0.072       -0.137   \n2786      -0.096  ...     0.026     0.021     0.002     0.007       -0.055   \n2787      -0.269  ...    -0.085    -0.028     0.021     0.030        0.034   \n2788      -0.056  ...     0.036     0.072     0.047     0.033       -0.191   \n2789      -0.334  ...    -0.218    -0.228    -0.159    -0.049        0.387   \n\n                     Date  Date.year  Date.month  Date.day  Date.dayofweek  \n2785  1654214400000000000       2022           6         3               4  \n2786  1654257600000000000       2022           6         3               4  \n2787  1654300800000000000       2022           6         4               5  \n2788  1654344000000000000       2022           6         4               5  \n2789  1654387200000000000       2022           6         5               6  \n\n[5 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>error_11</th>\n      <th>error_12</th>\n      <th>error_13</th>\n      <th>error_14</th>\n      <th>day_8_error</th>\n      <th>Date</th>\n      <th>Date.year</th>\n      <th>Date.month</th>\n      <th>Date.day</th>\n      <th>Date.dayofweek</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2785</th>\n      <td>-0.324</td>\n      <td>-0.276</td>\n      <td>-0.110</td>\n      <td>-0.182</td>\n      <td>-0.039</td>\n      <td>0.143</td>\n      <td>0.272</td>\n      <td>0.178</td>\n      <td>0.121</td>\n      <td>0.060</td>\n      <td>...</td>\n      <td>0.088</td>\n      <td>0.016</td>\n      <td>-0.049</td>\n      <td>-0.072</td>\n      <td>-0.137</td>\n      <td>1654214400000000000</td>\n      <td>2022</td>\n      <td>6</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2786</th>\n      <td>-0.233</td>\n      <td>-0.238</td>\n      <td>-0.207</td>\n      <td>-0.074</td>\n      <td>0.055</td>\n      <td>0.106</td>\n      <td>-0.070</td>\n      <td>-0.170</td>\n      <td>-0.144</td>\n      <td>-0.096</td>\n      <td>...</td>\n      <td>0.026</td>\n      <td>0.021</td>\n      <td>0.002</td>\n      <td>0.007</td>\n      <td>-0.055</td>\n      <td>1654257600000000000</td>\n      <td>2022</td>\n      <td>6</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2787</th>\n      <td>-0.164</td>\n      <td>-0.103</td>\n      <td>-0.039</td>\n      <td>0.044</td>\n      <td>0.033</td>\n      <td>0.025</td>\n      <td>0.201</td>\n      <td>-0.113</td>\n      <td>-0.270</td>\n      <td>-0.269</td>\n      <td>...</td>\n      <td>-0.085</td>\n      <td>-0.028</td>\n      <td>0.021</td>\n      <td>0.030</td>\n      <td>0.034</td>\n      <td>1654300800000000000</td>\n      <td>2022</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2788</th>\n      <td>-0.224</td>\n      <td>-0.029</td>\n      <td>-0.032</td>\n      <td>-0.087</td>\n      <td>-0.065</td>\n      <td>0.020</td>\n      <td>-0.119</td>\n      <td>-0.236</td>\n      <td>-0.202</td>\n      <td>-0.056</td>\n      <td>...</td>\n      <td>0.036</td>\n      <td>0.072</td>\n      <td>0.047</td>\n      <td>0.033</td>\n      <td>-0.191</td>\n      <td>1654344000000000000</td>\n      <td>2022</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2789</th>\n      <td>0.238</td>\n      <td>0.247</td>\n      <td>0.328</td>\n      <td>0.372</td>\n      <td>0.151</td>\n      <td>-0.013</td>\n      <td>-0.160</td>\n      <td>-0.389</td>\n      <td>-0.462</td>\n      <td>-0.334</td>\n      <td>...</td>\n      <td>-0.218</td>\n      <td>-0.228</td>\n      <td>-0.159</td>\n      <td>-0.049</td>\n      <td>0.387</td>\n      <td>1654387200000000000</td>\n      <td>2022</td>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 26 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_predictor = MultilabelPredictor.load(save_path)\n",
    "test_data_nolab = test_data.drop(columns=labels)\n",
    "test_data_nolab.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:42.534680Z",
     "end_time": "2023-06-07T12:29:42.539580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with TabularPredictor for label: ecmwf-eps_9 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_10 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_11 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_12 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_13 ...\n",
      "Predicting with TabularPredictor for label: ecmwf-eps_14 ...\n",
      "Predictions:  \n",
      "       ecmwf-eps_9  ecmwf-eps_10  ecmwf-eps_11  ecmwf-eps_12  ecmwf-eps_13  \\\n",
      "2785    -0.127101     -0.126792     -0.097127     -0.141238     -0.043186   \n",
      "2786    -0.116464     -0.123571     -0.162110     -0.069101      0.040029   \n",
      "2787     0.018806     -0.007055     -0.057805     -0.013736      0.015636   \n",
      "2788    -0.188968     -0.071285     -0.120150     -0.131894     -0.028873   \n",
      "2789     0.234517      0.059488      0.021974      0.073117      0.025593   \n",
      "...           ...           ...           ...           ...           ...   \n",
      "3477     1.353362      1.054642      0.258455      0.153893      0.098794   \n",
      "3478    -0.457566      0.014786      0.220862      0.054459      0.018730   \n",
      "3479     0.241550      0.133205      0.273329      0.242818      0.270754   \n",
      "3480    -0.539933     -0.346235     -0.100893     -0.015253      0.129454   \n",
      "3481     0.363347      0.193944      0.226445      0.003179     -0.181453   \n",
      "\n",
      "      ecmwf-eps_14  \n",
      "2785      0.070665  \n",
      "2786      0.118516  \n",
      "2787      0.043273  \n",
      "2788      0.104228  \n",
      "2789     -0.022895  \n",
      "...            ...  \n",
      "3477      0.168778  \n",
      "3478      0.179074  \n",
      "3479      0.322647  \n",
      "3480      0.321857  \n",
      "3481     -0.051098  \n",
      "\n",
      "[697 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions = multi_predictor.predict(test_data_nolab)\n",
    "print(\"Predictions:  \\n\", predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:42.539962Z",
     "end_time": "2023-06-07T12:29:42.803880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "ecmwf-eps_9     0.363347\necmwf-eps_10    0.193944\necmwf-eps_11    0.226445\necmwf-eps_12    0.003179\necmwf-eps_13   -0.181453\necmwf-eps_14   -0.051098\nName: 3481, dtype: float32"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.iloc[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:42.806856Z",
     "end_time": "2023-06-07T12:29:42.811263Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "ecmwf-eps_9     0.482\necmwf-eps_10    0.299\necmwf-eps_11    0.191\necmwf-eps_12   -0.095\necmwf-eps_13   -0.359\necmwf-eps_14   -0.209\nName: 3481, dtype: float64"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[labels].iloc[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:42.811446Z",
     "end_time": "2023-06-07T12:29:42.815071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.6090736574597795\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6090736574597795,\n",
      "    \"mean_squared_error\": -0.3709707202114328,\n",
      "    \"mean_absolute_error\": -0.3902446393561383,\n",
      "    \"r2\": 0.8494874320546517,\n",
      "    \"pearsonr\": 0.9220851690553549,\n",
      "    \"median_absolute_error\": -0.22452440834045362\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_9 ...\n",
      "Evaluating TabularPredictor for label: ecmwf-eps_10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.8548174638655262\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.8548174638655262,\n",
      "    \"mean_squared_error\": -0.7307128965294901,\n",
      "    \"mean_absolute_error\": -0.5851445703134903,\n",
      "    \"r2\": 0.6747954353689847,\n",
      "    \"pearsonr\": 0.8221657400739759,\n",
      "    \"median_absolute_error\": -0.3247323484420761\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -1.0436577367552093\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.0436577367552093,\n",
      "    \"mean_squared_error\": -1.0892214714890056,\n",
      "    \"mean_absolute_error\": -0.7145156305984719,\n",
      "    \"r2\": 0.4148432653122116,\n",
      "    \"pearsonr\": 0.6689676080239068,\n",
      "    \"median_absolute_error\": -0.4271272525787353\n",
      "}\n",
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -1.0621893798149786\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.0621893798149786,\n",
      "    \"mean_squared_error\": -1.128246278591729,\n",
      "    \"mean_absolute_error\": -0.7213494947236219,\n",
      "    \"r2\": 0.27029332647959514,\n",
      "    \"pearsonr\": 0.5852141111413277,\n",
      "    \"median_absolute_error\": -0.39760993194579974\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_11 ...\n",
      "Evaluating TabularPredictor for label: ecmwf-eps_12 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.9978911183051414\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9978911183051414,\n",
      "    \"mean_squared_error\": -0.9957866839922858,\n",
      "    \"mean_absolute_error\": -0.6772496321475404,\n",
      "    \"r2\": 0.21626515466658291,\n",
      "    \"pearsonr\": 0.5097039678551379,\n",
      "    \"median_absolute_error\": -0.3883330336809152\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TabularPredictor for label: ecmwf-eps_13 ...\n",
      "Evaluating TabularPredictor for label: ecmwf-eps_14 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Calling `predictor.predict_proba` when problem_type=regression will raise an AssertionError starting in AutoGluon v0.8. Please call `predictor.predict` instead.\n",
      "Evaluation: root_mean_squared_error on test data: -0.9403125556890998\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9403125556890998,\n",
      "    \"mean_squared_error\": -0.8841877023865663,\n",
      "    \"mean_absolute_error\": -0.6377953638311588,\n",
      "    \"r2\": 0.18864241275200255,\n",
      "    \"pearsonr\": 0.46767044187488943,\n",
      "    \"median_absolute_error\": -0.37786263179778956\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated using metrics: {'ecmwf-eps_9': root_mean_squared_error, 'ecmwf-eps_10': root_mean_squared_error, 'ecmwf-eps_11': root_mean_squared_error, 'ecmwf-eps_12': root_mean_squared_error, 'ecmwf-eps_13': root_mean_squared_error, 'ecmwf-eps_14': root_mean_squared_error}\n"
     ]
    }
   ],
   "source": [
    "evaluations = multi_predictor.evaluate(test_data)\n",
    "#print(evaluations)\n",
    "print(\"Evaluated using metrics:\", multi_predictor.eval_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:42.817011Z",
     "end_time": "2023-06-07T12:29:43.354717Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ecmwf-eps_9': {'root_mean_squared_error': -0.6090736574597795,\n  'mean_squared_error': -0.3709707202114328,\n  'mean_absolute_error': -0.3902446393561383,\n  'r2': 0.8494874320546517,\n  'pearsonr': 0.9220851690553549,\n  'median_absolute_error': -0.22452440834045362},\n 'ecmwf-eps_10': {'root_mean_squared_error': -0.8548174638655262,\n  'mean_squared_error': -0.7307128965294901,\n  'mean_absolute_error': -0.5851445703134903,\n  'r2': 0.6747954353689847,\n  'pearsonr': 0.8221657400739759,\n  'median_absolute_error': -0.3247323484420761},\n 'ecmwf-eps_11': {'root_mean_squared_error': -1.0436577367552093,\n  'mean_squared_error': -1.0892214714890056,\n  'mean_absolute_error': -0.7145156305984719,\n  'r2': 0.4148432653122116,\n  'pearsonr': 0.6689676080239068,\n  'median_absolute_error': -0.4271272525787353},\n 'ecmwf-eps_12': {'root_mean_squared_error': -1.0621893798149786,\n  'mean_squared_error': -1.128246278591729,\n  'mean_absolute_error': -0.7213494947236219,\n  'r2': 0.27029332647959514,\n  'pearsonr': 0.5852141111413277,\n  'median_absolute_error': -0.39760993194579974},\n 'ecmwf-eps_13': {'root_mean_squared_error': -0.9978911183051414,\n  'mean_squared_error': -0.9957866839922858,\n  'mean_absolute_error': -0.6772496321475404,\n  'r2': 0.21626515466658291,\n  'pearsonr': 0.5097039678551379,\n  'median_absolute_error': -0.3883330336809152},\n 'ecmwf-eps_14': {'root_mean_squared_error': -0.9403125556890998,\n  'mean_squared_error': -0.8841877023865663,\n  'mean_absolute_error': -0.6377953638311588,\n  'r2': 0.18864241275200255,\n  'pearsonr': 0.46767044187488943,\n  'median_absolute_error': -0.37786263179778956}}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:43.354516Z",
     "end_time": "2023-06-07T12:29:43.364427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                 model  score_val  pred_time_val  fit_time  \\\n0  WeightedEnsemble_L2  -0.573329       0.011542  9.501996   \n1      NeuralNetFastAI  -0.592686       0.006101  1.628658   \n2       NeuralNetTorch  -0.598182       0.003432  3.944523   \n3             CatBoost  -0.629614       0.001771  3.799893   \n4              XGBoost  -0.687358       0.005066  2.407276   \n5        ExtraTreesMSE  -0.707621       0.026708  0.606895   \n6      RandomForestMSE  -0.719315       0.027498  2.504079   \n7       KNeighborsUnif  -1.262602       0.005954  0.005703   \n8       KNeighborsDist  -1.379897       0.008375  0.006608   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.000238           0.128922            2       True   \n1                0.006101           1.628658            1       True   \n2                0.003432           3.944523            1       True   \n3                0.001771           3.799893            1       True   \n4                0.005066           2.407276            1       True   \n5                0.026708           0.606895            1       True   \n6                0.027498           2.504079            1       True   \n7                0.005954           0.005703            1       True   \n8                0.008375           0.006608            1       True   \n\n   fit_order  \n0          9  \n1          6  \n2          8  \n3          4  \n4          7  \n5          5  \n6          3  \n7          1  \n8          2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_val</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>-0.573329</td>\n      <td>0.011542</td>\n      <td>9.501996</td>\n      <td>0.000238</td>\n      <td>0.128922</td>\n      <td>2</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NeuralNetFastAI</td>\n      <td>-0.592686</td>\n      <td>0.006101</td>\n      <td>1.628658</td>\n      <td>0.006101</td>\n      <td>1.628658</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NeuralNetTorch</td>\n      <td>-0.598182</td>\n      <td>0.003432</td>\n      <td>3.944523</td>\n      <td>0.003432</td>\n      <td>3.944523</td>\n      <td>1</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CatBoost</td>\n      <td>-0.629614</td>\n      <td>0.001771</td>\n      <td>3.799893</td>\n      <td>0.001771</td>\n      <td>3.799893</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGBoost</td>\n      <td>-0.687358</td>\n      <td>0.005066</td>\n      <td>2.407276</td>\n      <td>0.005066</td>\n      <td>2.407276</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ExtraTreesMSE</td>\n      <td>-0.707621</td>\n      <td>0.026708</td>\n      <td>0.606895</td>\n      <td>0.026708</td>\n      <td>0.606895</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RandomForestMSE</td>\n      <td>-0.719315</td>\n      <td>0.027498</td>\n      <td>2.504079</td>\n      <td>0.027498</td>\n      <td>2.504079</td>\n      <td>1</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>KNeighborsUnif</td>\n      <td>-1.262602</td>\n      <td>0.005954</td>\n      <td>0.005703</td>\n      <td>0.005954</td>\n      <td>0.005703</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>KNeighborsDist</td>\n      <td>-1.379897</td>\n      <td>0.008375</td>\n      <td>0.006608</td>\n      <td>0.008375</td>\n      <td>0.006608</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_class = multi_predictor.get_predictor('ecmwf-eps_13')\n",
    "predictor_class.leaderboard(silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T12:29:43.359552Z",
     "end_time": "2023-06-07T12:29:43.412753Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "feature importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "day = 2\n",
    "df = train_data.iloc[:, :-5+day]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T13:08:40.662186Z",
     "end_time": "2023-06-07T13:08:40.665494Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "   gfs-ens-bc_9  gfs-ens-bc_10  gfs-ens-bc_11  gfs-ens-bc_12  gfs-ens-bc_13  \\\n0         0.012          0.009          0.006          0.002          0.004   \n1         0.001          0.001          0.004          0.009          0.011   \n2         0.010          0.007          0.011          0.013          0.011   \n3         0.006          0.009          0.010          0.009          0.009   \n4         0.012          0.007          0.008          0.007          0.009   \n\n   gfs-ens-bc_14  cmc-ens_9  cmc-ens_10  cmc-ens_11  cmc-ens_12  ...  \\\n0          0.011     -0.008      -0.005      -0.001      -0.003  ...   \n1          0.013     -0.003      -0.002      -0.004      -0.008  ...   \n2          0.008     -0.007      -0.011      -0.013      -0.011  ...   \n3          0.009     -0.008      -0.009      -0.009      -0.010  ...   \n4          0.009     -0.007      -0.009      -0.009      -0.010  ...   \n\n   error_14  day_8_error                 Date  Date.year  Date.month  \\\n0     0.000        0.000  1531267200000000000       2018           7   \n1    -0.001        0.005  1531310400000000000       2018           7   \n2     0.000        0.000  1531353600000000000       2018           7   \n3    -0.001       -0.002  1531396800000000000       2018           7   \n4     0.001        0.000  1531440000000000000       2018           7   \n\n   Date.day  Date.dayofweek  ecmwf-eps_9  ecmwf-eps_10  ecmwf-eps_11  \n0        11               2        0.000         0.002         0.001  \n1        11               2        0.001         0.000         0.000  \n2        12               3        0.000         0.003         0.001  \n3        12               3       -0.003        -0.001         0.001  \n4        13               4        0.000         0.000        -0.002  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gfs-ens-bc_9</th>\n      <th>gfs-ens-bc_10</th>\n      <th>gfs-ens-bc_11</th>\n      <th>gfs-ens-bc_12</th>\n      <th>gfs-ens-bc_13</th>\n      <th>gfs-ens-bc_14</th>\n      <th>cmc-ens_9</th>\n      <th>cmc-ens_10</th>\n      <th>cmc-ens_11</th>\n      <th>cmc-ens_12</th>\n      <th>...</th>\n      <th>error_14</th>\n      <th>day_8_error</th>\n      <th>Date</th>\n      <th>Date.year</th>\n      <th>Date.month</th>\n      <th>Date.day</th>\n      <th>Date.dayofweek</th>\n      <th>ecmwf-eps_9</th>\n      <th>ecmwf-eps_10</th>\n      <th>ecmwf-eps_11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.012</td>\n      <td>0.009</td>\n      <td>0.006</td>\n      <td>0.002</td>\n      <td>0.004</td>\n      <td>0.011</td>\n      <td>-0.008</td>\n      <td>-0.005</td>\n      <td>-0.001</td>\n      <td>-0.003</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>1531267200000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>11</td>\n      <td>2</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.004</td>\n      <td>0.009</td>\n      <td>0.011</td>\n      <td>0.013</td>\n      <td>-0.003</td>\n      <td>-0.002</td>\n      <td>-0.004</td>\n      <td>-0.008</td>\n      <td>...</td>\n      <td>-0.001</td>\n      <td>0.005</td>\n      <td>1531310400000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>11</td>\n      <td>2</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.010</td>\n      <td>0.007</td>\n      <td>0.011</td>\n      <td>0.013</td>\n      <td>0.011</td>\n      <td>0.008</td>\n      <td>-0.007</td>\n      <td>-0.011</td>\n      <td>-0.013</td>\n      <td>-0.011</td>\n      <td>...</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>1531353600000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>12</td>\n      <td>3</td>\n      <td>0.000</td>\n      <td>0.003</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.006</td>\n      <td>0.009</td>\n      <td>0.010</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.008</td>\n      <td>-0.009</td>\n      <td>-0.009</td>\n      <td>-0.010</td>\n      <td>...</td>\n      <td>-0.001</td>\n      <td>-0.002</td>\n      <td>1531396800000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>12</td>\n      <td>3</td>\n      <td>-0.003</td>\n      <td>-0.001</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.012</td>\n      <td>0.007</td>\n      <td>0.008</td>\n      <td>0.007</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.007</td>\n      <td>-0.009</td>\n      <td>-0.009</td>\n      <td>-0.010</td>\n      <td>...</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>1531440000000000000</td>\n      <td>2018</td>\n      <td>7</td>\n      <td>13</td>\n      <td>4</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>-0.002</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 29 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T13:08:41.118230Z",
     "end_time": "2023-06-07T13:08:41.135028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "label = f\"ecmwf-eps_{day+9}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T13:08:41.285270Z",
     "end_time": "2023-06-07T13:08:41.289012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230607_190841/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230607_190841/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Mon Apr 24 20:53:44 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T8103\n",
      "Train Data Rows:    2785\n",
      "Train Data Columns: 28\n",
      "Label Column: ecmwf-eps_11\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.802, -7.863000000000003, 0.02167, 1.37127)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1366.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.62 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 23 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 23 | ['gfs-ens-bc_9', 'gfs-ens-bc_10', 'gfs-ens-bc_11', 'gfs-ens-bc_12', 'gfs-ens-bc_13', ...]\n",
      "\t\t('int', [])   :  5 | ['Date', 'Date.year', 'Date.month', 'Date.day', 'Date.dayofweek']\n",
      "\t0.0s = Fit runtime\n",
      "\t28 features in original data used to generate 28 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.62 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.17953321364452424, Train Rows: 2285, Val Rows: 500\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-1.5792\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-1.7234\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.8561\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.37s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.7826\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.8667\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.6875\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.8502\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.6789\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <D21A7969-4567-3BC7-94ED-6A9E83AE9D78> /Users/nickgault/opt/miniconda3/envs/Kayak/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.6586\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 18.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230607_190841/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label).fit(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T13:08:41.936834Z",
     "end_time": "2023-06-07T13:09:00.215411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['ecmwf-eps_12', 'ecmwf-eps_13', 'ecmwf-eps_14']\n",
      "Computing feature importance via permutation shuffling for 28 features using 697 rows with 5 shuffle sets...\n",
      "\t3.12s\t= Expected runtime (0.62s per shuffle set)\n",
      "\t1.08s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "shap_values = predictor.feature_importance(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T13:09:00.216683Z",
     "end_time": "2023-06-07T13:09:01.309607Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "                importance    stddev       p_value  n  p99_high   p99_low\necmwf-eps_10      1.360374  0.021828  7.951130e-09  5  1.405318  1.315431\ngfs-ens-bc_11     1.028955  0.022470  2.727290e-08  5  1.075221  0.982689\ngfs-ens-bc_10     0.993484  0.034132  1.669213e-07  5  1.063762  0.923205\necmwf-eps_9       0.294865  0.019340  2.207935e-06  5  0.334686  0.255045\ncmc-ens_11        0.165934  0.006879  3.535929e-07  5  0.180097  0.151770\ngfs-ens-bc_9      0.076887  0.009178  2.390719e-05  5  0.095785  0.057990\ngfs-ens-bc_13     0.052920  0.001326  4.732346e-08  5  0.055652  0.050189\ngfs-ens-bc_12     0.047426  0.006449  4.004073e-05  5  0.060705  0.034147\ncmc-ens_9         0.018814  0.002782  5.574089e-05  5  0.024542  0.013085\ngfs-ens-bc_14     0.014203  0.002489  1.087266e-04  5  0.019329  0.009078\nday_8_error       0.010008  0.004360  3.411554e-03  5  0.018985  0.001032\nerror_14          0.008790  0.002890  1.220958e-03  5  0.014740  0.002839\necmwf_diff_8      0.007188  0.003127  3.395068e-03  5  0.013626  0.000750\ncmc-ens_10        0.005701  0.002243  2.367243e-03  5  0.010319  0.001082\nerror_10          0.004160  0.003637  3.140299e-02  5  0.011648 -0.003329\nDate.year         0.003834  0.000847  2.675904e-04  5  0.005577  0.002091\nDate              0.001782  0.000787  3.589231e-03  5  0.003403  0.000161\nerror_9           0.001206  0.003061  2.140821e-01  5  0.007508 -0.005097\nerror_11          0.001152  0.005046  3.182885e-01  5  0.011541 -0.009237\nerror_12          0.000849  0.004591  3.502244e-01  5  0.010301 -0.008603\nerror_13          0.000169  0.002529  4.440941e-01  5  0.005376 -0.005037\ncmc-ens_14        0.000142  0.004041  4.706013e-01  5  0.008462 -0.008178\nDate.month       -0.000802  0.002796  7.218639e-01  5  0.004955 -0.006559\nDate.dayofweek   -0.003375  0.003169  9.620410e-01  5  0.003151 -0.009901\nDate.day         -0.004800  0.003073  9.874632e-01  5  0.001528 -0.011128\ncmc-ens_13       -0.004812  0.004937  9.525858e-01  5  0.005354 -0.014977\necmwf_diff_9     -0.006255  0.003900  9.884779e-01  5  0.001776 -0.014286\ncmc-ens_12       -0.007923  0.003374  9.968545e-01  5 -0.000977 -0.014870",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n      <th>stddev</th>\n      <th>p_value</th>\n      <th>n</th>\n      <th>p99_high</th>\n      <th>p99_low</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ecmwf-eps_10</th>\n      <td>1.360374</td>\n      <td>0.021828</td>\n      <td>7.951130e-09</td>\n      <td>5</td>\n      <td>1.405318</td>\n      <td>1.315431</td>\n    </tr>\n    <tr>\n      <th>gfs-ens-bc_11</th>\n      <td>1.028955</td>\n      <td>0.022470</td>\n      <td>2.727290e-08</td>\n      <td>5</td>\n      <td>1.075221</td>\n      <td>0.982689</td>\n    </tr>\n    <tr>\n      <th>gfs-ens-bc_10</th>\n      <td>0.993484</td>\n      <td>0.034132</td>\n      <td>1.669213e-07</td>\n      <td>5</td>\n      <td>1.063762</td>\n      <td>0.923205</td>\n    </tr>\n    <tr>\n      <th>ecmwf-eps_9</th>\n      <td>0.294865</td>\n      <td>0.019340</td>\n      <td>2.207935e-06</td>\n      <td>5</td>\n      <td>0.334686</td>\n      <td>0.255045</td>\n    </tr>\n    <tr>\n      <th>cmc-ens_11</th>\n      <td>0.165934</td>\n      <td>0.006879</td>\n      <td>3.535929e-07</td>\n      <td>5</td>\n      <td>0.180097</td>\n      <td>0.151770</td>\n    </tr>\n    <tr>\n      <th>gfs-ens-bc_9</th>\n      <td>0.076887</td>\n      <td>0.009178</td>\n      <td>2.390719e-05</td>\n      <td>5</td>\n      <td>0.095785</td>\n      <td>0.057990</td>\n    </tr>\n    <tr>\n      <th>gfs-ens-bc_13</th>\n      <td>0.052920</td>\n      <td>0.001326</td>\n      <td>4.732346e-08</td>\n      <td>5</td>\n      <td>0.055652</td>\n      <td>0.050189</td>\n    </tr>\n    <tr>\n      <th>gfs-ens-bc_12</th>\n      <td>0.047426</td>\n      <td>0.006449</td>\n      <td>4.004073e-05</td>\n      <td>5</td>\n      <td>0.060705</td>\n      <td>0.034147</td>\n    </tr>\n    <tr>\n      <th>cmc-ens_9</th>\n      <td>0.018814</td>\n      <td>0.002782</td>\n      <td>5.574089e-05</td>\n      <td>5</td>\n      <td>0.024542</td>\n      <td>0.013085</td>\n    </tr>\n    <tr>\n      <th>gfs-ens-bc_14</th>\n      <td>0.014203</td>\n      <td>0.002489</td>\n      <td>1.087266e-04</td>\n      <td>5</td>\n      <td>0.019329</td>\n      <td>0.009078</td>\n    </tr>\n    <tr>\n      <th>day_8_error</th>\n      <td>0.010008</td>\n      <td>0.004360</td>\n      <td>3.411554e-03</td>\n      <td>5</td>\n      <td>0.018985</td>\n      <td>0.001032</td>\n    </tr>\n    <tr>\n      <th>error_14</th>\n      <td>0.008790</td>\n      <td>0.002890</td>\n      <td>1.220958e-03</td>\n      <td>5</td>\n      <td>0.014740</td>\n      <td>0.002839</td>\n    </tr>\n    <tr>\n      <th>ecmwf_diff_8</th>\n      <td>0.007188</td>\n      <td>0.003127</td>\n      <td>3.395068e-03</td>\n      <td>5</td>\n      <td>0.013626</td>\n      <td>0.000750</td>\n    </tr>\n    <tr>\n      <th>cmc-ens_10</th>\n      <td>0.005701</td>\n      <td>0.002243</td>\n      <td>2.367243e-03</td>\n      <td>5</td>\n      <td>0.010319</td>\n      <td>0.001082</td>\n    </tr>\n    <tr>\n      <th>error_10</th>\n      <td>0.004160</td>\n      <td>0.003637</td>\n      <td>3.140299e-02</td>\n      <td>5</td>\n      <td>0.011648</td>\n      <td>-0.003329</td>\n    </tr>\n    <tr>\n      <th>Date.year</th>\n      <td>0.003834</td>\n      <td>0.000847</td>\n      <td>2.675904e-04</td>\n      <td>5</td>\n      <td>0.005577</td>\n      <td>0.002091</td>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <td>0.001782</td>\n      <td>0.000787</td>\n      <td>3.589231e-03</td>\n      <td>5</td>\n      <td>0.003403</td>\n      <td>0.000161</td>\n    </tr>\n    <tr>\n      <th>error_9</th>\n      <td>0.001206</td>\n      <td>0.003061</td>\n      <td>2.140821e-01</td>\n      <td>5</td>\n      <td>0.007508</td>\n      <td>-0.005097</td>\n    </tr>\n    <tr>\n      <th>error_11</th>\n      <td>0.001152</td>\n      <td>0.005046</td>\n      <td>3.182885e-01</td>\n      <td>5</td>\n      <td>0.011541</td>\n      <td>-0.009237</td>\n    </tr>\n    <tr>\n      <th>error_12</th>\n      <td>0.000849</td>\n      <td>0.004591</td>\n      <td>3.502244e-01</td>\n      <td>5</td>\n      <td>0.010301</td>\n      <td>-0.008603</td>\n    </tr>\n    <tr>\n      <th>error_13</th>\n      <td>0.000169</td>\n      <td>0.002529</td>\n      <td>4.440941e-01</td>\n      <td>5</td>\n      <td>0.005376</td>\n      <td>-0.005037</td>\n    </tr>\n    <tr>\n      <th>cmc-ens_14</th>\n      <td>0.000142</td>\n      <td>0.004041</td>\n      <td>4.706013e-01</td>\n      <td>5</td>\n      <td>0.008462</td>\n      <td>-0.008178</td>\n    </tr>\n    <tr>\n      <th>Date.month</th>\n      <td>-0.000802</td>\n      <td>0.002796</td>\n      <td>7.218639e-01</td>\n      <td>5</td>\n      <td>0.004955</td>\n      <td>-0.006559</td>\n    </tr>\n    <tr>\n      <th>Date.dayofweek</th>\n      <td>-0.003375</td>\n      <td>0.003169</td>\n      <td>9.620410e-01</td>\n      <td>5</td>\n      <td>0.003151</td>\n      <td>-0.009901</td>\n    </tr>\n    <tr>\n      <th>Date.day</th>\n      <td>-0.004800</td>\n      <td>0.003073</td>\n      <td>9.874632e-01</td>\n      <td>5</td>\n      <td>0.001528</td>\n      <td>-0.011128</td>\n    </tr>\n    <tr>\n      <th>cmc-ens_13</th>\n      <td>-0.004812</td>\n      <td>0.004937</td>\n      <td>9.525858e-01</td>\n      <td>5</td>\n      <td>0.005354</td>\n      <td>-0.014977</td>\n    </tr>\n    <tr>\n      <th>ecmwf_diff_9</th>\n      <td>-0.006255</td>\n      <td>0.003900</td>\n      <td>9.884779e-01</td>\n      <td>5</td>\n      <td>0.001776</td>\n      <td>-0.014286</td>\n    </tr>\n    <tr>\n      <th>cmc-ens_12</th>\n      <td>-0.007923</td>\n      <td>0.003374</td>\n      <td>9.968545e-01</td>\n      <td>5</td>\n      <td>-0.000977</td>\n      <td>-0.014870</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-07T13:09:01.316819Z",
     "end_time": "2023-06-07T13:09:01.319692Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
